\chapter{Introduction}
We consider the symmetric eigenvalue problem, \ie given a symmetric
matrix $\mat{A} \in \R^{n \times n}$ are looking for
$0 \neq \vec{v} \in \R^{n}$ and $\lambda \in \R$ such that
\begin{equation}
  \label{eq:eigenproblem}
  \mat{A}\vec{v} = \lambda \vec{v} \,.
\end{equation}
We assume that $\mat{A}$ is positive definite which implies, since
$\mat{A}$ is also symmetric, that all eigenvalues of $\mat{A}$ are
positive. In the following we are especially interested in finding
\emph{interior} eigenpairs of $\mat{A}$. By this we mean that if the
eigenvalues of $\mat{A}$ are labeled in increasing order of magnitude
$\lambda_1 \le \lambda_2 \le \dotsc \le \lambda_n$ we are looking for
$(\lambda_k, \vec{v}_k)$ where $1 < k < n$.

\begin{example}[Discretization of the LaPlacian]
  This example describes an eigenvalue problem that naturally arises
  in the investigation of resonance frequencies\footnote{This example
    is adapted from~\cite[1--3]{borm2012}}. We consider the
  oscillations of a string of unit length and represent the string by
  a function
  \[
    u : [0,1] \times \R \rightarrow \R, \qquad (x,t) \mapsto u(x,t)
  \]
  where $u(x,t)$ models the displacement of the string at time $t$ in
  the $y$ direction of a point originally at position $x$ (if we
  assume the string to lie in the $x-y$-plane). We assume the string
  is fixed at both of is ends.  We want analyse the strings resonance
  frequencies, \ie we want to find \emph{standing waves}. To this end,
  consider the following set of equations
  \begin{subequations}%
    \label{eq:wave:bvp}
    \begin{align}
      \frac{\partial^2 u}{\partial x^2}(x,t) - c^2 \frac{\partial^2 u}{\partial t^2}(x,t) &= 0 && \text{for all } t \in \R \text{ and } x \in (0,1)\,, \label{eq:wave:bvp:eq} \\
      % u(0,x) = u_0(x) \,, \label{eq:wave:bvp:iv1} \\
      % \frac{\partial u}{\partial t}u(0,x) = 0
      % \,, \label{eq:wave:bvp:iv2} \\
      u(t,0) = u(t,1) &= 0 && \text{for all } t \in \R \,, \label{eq:wave:bvp:bv}
    \end{align}
  \end{subequations}
  where $c > 0$ is a paramater describing the string's
  properties. Equation~\eqref{eq:wave:bvp:eq} is called the \emph{wave
    equation} in one dimension. The boundary
  conditions~\eqref{eq:wave:bvp:bv} encode the fact that the string is fixed at its ends.
  Since we try to find standing waves of the string we can also 
  To solve this partial differential equation
  (PDE) we will use use separation of variables to make the following
  ansatz. Assume the solution is of the form $u(x,t) = X(x)T(t)$. Equation~\eqref{eq:wave:bvp:eq} becomes
  \[
    
  \]




  
  We can now rewrite this problem as an eigenvalue problem. To this
  end, we define the differential operator $L$ for $u \in \CC^2(0,1)$
  by
  \[
    L[u](x) \coloneqq -c u^{\prime \prime}(x) \quad \text{for all } x
    \in (0,1) \,.
  \]
  We obtain the following eigenvalue problem
  \begin{equation}
    \label{eq:diffop:eigenvalue}
    L[u_0] = \lambda u_0
  \end{equation}
  on the inifinte-dimensional space $\CC^2(0,1)$. In order to solve
  this problem numerically, we have to discretize it. Taylor expansion
  of $u_0$ around some $x \in (0,1)$ yields the \emph{central finite
    difference quotient}
  \[
    u_0^{\prime \prime}(x) = \frac{u_0(x+h) - 2u_0(x) + u_0(x-h)}{h^2}
    + \mathcal{O}(h^2).
  \]
  If we only consider $u_0$ in the discrete mesh points $x_i = ih$,
  $i = 0, \dotsc, m$, for $h = m^{-1}$ and neglect the
  $\mathcal{O}(h^2)$ term we obtain
  \[
    u^{\prime \prime}_0(x_i) \approx \frac{1}{h^2} \left( u_0(x_{i-1})
      - 2u_0(x_i) + u_0(x_{i+1}) \right) \quad \text{for all } i = 1,
    \dotsc, m-1 \,.
  \]
  This allows us to approximate the left-hand side
  of~\eqref{eq:diffop:eigenvalue} and we get
  \begin{equation}
    \label{eq:poisson:approx}
    \frac{c}{h^2}\left( 2u_0(x_i) - u_0(x_{i+1}) - u_0(x_{i+1}) \right) \approx \lambda u_0(x_i) \quad \text{for all } i = 1, \dotsc, m-1 \,.
  \end{equation}
  From the boundary conditions in~\eqref{eq:wave:bvp:bv} we already
  know $u_0(x_0) = u_0(x_{m}) = 0$.  This allows us to
  rewrite~\eqref{eq:diffop:eigenvalue} to obtain the following
  algebraic eigenvalue problem
  \begin{equation}
    \label{eq:poisson:sle}
    \mat{A} \vec{u} \coloneqq \frac{c}{h^2}
    \begin{pmatrix}
      2      & -1     &        &        &       \\
      -1     & 2      & -1     &        &       \\
      & \ddots & \ddots & \ddots &       \\
      &        & \ddots & \ddots & -1    \\
      & & & -1 & 2
    \end{pmatrix}
    \begin{pmatrix}
      u_0(x_1) \\ u_0(x_2) \\ \vdots \\ u_0(x_{m-1}) \\ u_0(x_m)
    \end{pmatrix}
    \approx
    \lambda
    \begin{pmatrix}
      u_0(x_1) \\ u_0(x_2) \\ \vdots \\ u_0(x_{m-1}) \\ u_0(x_m)
    \end{pmatrix}
  \end{equation}
  Assumming $c = 1$ the eigenvalues and eigenvectors of $\mat{A}$ can
  be explicitly determined (cf.~Appendix)
  \begin{equation}
    \label{eq:laplace:eigenpairs}
    \lambda_k = 4m^2 \sin^2 \left( \frac{k \pi}{2m} \right) \qquad
    \text{and} \qquad v_k = {\left( \sin \left( \frac{k \pi j}{m}
        \right) \right)}_{j=1}^{m-1} \,.
  \end{equation}
  In the following we will introduce a numerical method to iteratively
  compute approximations of the eigenpairs. We will, however, need
  approximations of the eigenvector that is to be computed. To that
  end we will use the exact values in~\eqref{eq:laplace:eigenpairs}
  and perturb them. This model problem
\end{example}

\section{Rayleigh quotient iteration}
In this section we introduce the Rayleigh quotient iteration, an
iterative method to compute approximations of eigenpairs of a matrix
$\mat{A}$, \ie to compute (approximate) soltuions
of~\eqref{eq:eigenproblem}. We start by revisiting the power method in
Algorithm~\ref{alg:power:method} before introducing the (shifted)
inverse iteration (Algorithm~\ref{alg:inverse:iteration}) which
directly leads us to the Rayleigh quotient iteration in
Algorithm~\ref{alg:rqi}.

The power method is based on generating the sequence
$\mat{A}^k \vec{v}_0$ where $\vec{v}_0$ is a non-zero vector. When
normalised appropriately this sequence convergences to the eigenvector
associated with the eigenvalue of largest modulus (under certain
resonable assumptions, cf.~\cite[Theorem 4.1, p.~86]{saad}). In
Algorithm~\ref{alg:power:method} we normalise by ensuring that the
largest component of the current approximation is equal to one.

\begin{algorithm}
  \DontPrintSemicolon
  \Begin{ Choose nonzero initial vector $\vec{v}_0$\;
    \For{$k = 1,2,\dotsc$ until convergence}{
      $\displaystyle \vec{v}_k =
      \frac{1}{\alpha_k}\mat{A}\vec{v}_{k-1}$\; \tcc{$\alpha_k$ is the
        component of $\mat{A}\vec{v}_{k-1}$ with the maximum modulus}
    } }
  \caption{Power method}\label{alg:power:method}
\end{algorithm}

The next algorithm makes use of the following two facts from linear
algebra.

\begin{lemma}
  Let $(\lambda, \vec{v})$ be an eigenpair of $\mat{A}$ and
  $\sigma \in \R$ an arbitrary scalar. Then
  \begin{enumerate}
  \item $\lambda - \sigma$ is an eigenvalue of
    $\mat{A} - \sigma \mat{I}$ with eigenvector $\vec{v}$.
  \item If $\mat{A}$ is nonsingular then $\lambda^{-1}$ is an
    eigenvalue of $\mat{A}^{-1}$ with eigenvector $\vec{v}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Since $\mat{A}\vec{v} = \lambda \vec{v}$, we have
  $\mat{A}\vec{v} - \sigma \vec{v} = \lambda \vec{v} - \sigma \vec{v}$
  and therefore
  $(\mat{A} - \sigma \mat{I})\vec{v} = (\lambda - \sigma)\vec{v}$,
  which proofs the first statement. For the second statement, we first
  remark that since $\mat{A}$ is nonsingular, $\lambda \neq
  0$. Therefore
  $\mat{A}^{-1}\mat{A}\vec{v} = \lambda \mat{A}^{-1}\vec{v}$, thus
  $\lambda^{-1} \vec{v} = \mat{A}^{-1} \vec{v}$.
\end{proof}

We now alter the power method by defining the iterate as follows
\begin{equation}
  \label{eq:inverse:iterate}
  \vec{v}_k = \frac{1}{\alpha_k} {(\mat{A} - \sigma \mat{I})}^{-1}
  \vec{v}_{k-1}\,.
\end{equation}
The method will then converge to the eigenvector corresponding to the
largest eigenvalue in modulus of ${(\mat{A} - \sigma \mat{I})}^{-1}$,
or to the one corresponding to the smallest eigenvalue of
$\mat{A} - \sigma \mat{I}$. We can make use of this fact to compute
interior eigenpairs of $\mat{A}$. To that end, consider the eigenvalue
$\lambda_k$, $1 < k < n$. We can choose a constant $\sigma \in \R$
such that
\[
  \alpha_1 = \frac{1}{\lambda_k - \sigma}
\]
is the dominant eigenvalue of ${(\mat{A} - \sigma \mat{I})}^{-1}$. In
other words, the method converges to the eigenvector corresponding to
the eigenvalue closest to $\sigma$. In practice, it is not necessary
to compute the inverse of $\mat{A} - \sigma \mat{I}$
explicitly. Instead, the $\mat{LU}$ decomposition is computed once
before the iteration. It is then sufficient to solve an upper and
lower triangular system in each step, reducing the computational
complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$. The method
is summarised in Algorithm~\ref{alg:inverse:iteration}.

\begin{algorithm}
  \DontPrintSemicolon
  \Begin{ Choose nonzero initial vector $\vec{v}_0$\; Compute
    $\mat{LU}$ decomposition of
    $\mat{A} - \sigma \mat{I} = \mat{LU}$\; \For{$k = 1,2,\dotsc$
      until convergence}{
      $\displaystyle \vec{v}_k = \frac{1}{\alpha_k}{(\mat{A} - \sigma
        \mat{I})}^{-1}\vec{v}_{k-1} = \frac{1}{\alpha_k} \mat{U}^{-1}
      \mat{L}^{-1}\vec{v}_{k-1}$\; \tcc{$\alpha_k$ is the component of
        ${(\mat{A} - \sigma \mat{I})}^{-1}\vec{v}_{k-1}$ with the
        maximum modulus} } }
  \caption{Shifted inverse iteration}\label{alg:inverse:iteration}
\end{algorithm}

The Rayleigh quotient iteration is now a straightforward modification
of this algorithm. Instead of using the same shift throughout every
iteration, we replace the shift by an approximation of the eigenvalue,
computed from the current iterate. To obtain such an approximation, we
try to find $\alpha \in \R$, such that
\[
  \norm{\mat{A} \vec{v} - \alpha \vec{v}}_2
\]
is minimized. This can be seen as a least squares problem of the form
$\vec{v} \alpha \approx \mat{A}\vec{v}$. The associated normal
equation is given by
$\vec{v}^\tp \vec{v} \alpha = \vec{v}^\tp \mat{A} \vec{v}$, or
\[
  \alpha = \frac{\vec{v}^\tp \mat{A} \vec{v}}{\vec{v}^\tp \vec{v}} \,.
\]
This result gives rise to the follwing definition.
\begin{definition}[Rayleigh quotient]
  Let $\mat{A} \in \R^{n \times n}$. The mapping
  \[
    \mathcal{R}_{\mat{A}} : \R^n \setminus \{0\} \rightarrow \R,
    \qquad x \mapsto \frac{\vec{x}^\tp \mat{A} \vec{x}}{\vec{x}^\tp
      \vec{x}}
  \]
  is called the \emph{Rayleigh quotient} corresponding to the matrix
  $\mat{A}$.
\end{definition}
Note, that for eigenvectors $\vec{v}$ of $\mat{A}$ to the eigenvalue
$\lambda$ the Rayleigh quotient yields
\[
  \mathcal{R}_{\mat{A}}(\vec{v}) = \frac{\vec{v}^\tp \lambda
    \vec{v}}{\vec{v}^\tp \vec{v}} = \lambda\,.
\]
We will now alter Algorithm~\ref{alg:inverse:iteration} by replacing
the shift with $\mathcal{R}_{\mat{A}}(\vec{v}_k)$. Also we replace the
normalisation process by dividing by the norm of the iterate, which is
already part of the computation of the Rayleigh quotient. Note that we
rewrite the nominator of the Rayleigh quotient by using the inner
product $\inp{\cdot}{\cdot}_2$.

\begin{algorithm}
  \DontPrintSemicolon
  \Begin{ Choose initial vector $\vec{v}_0$ such that
    $\norm{\vec{v}_0}_2 = 1$\; \For{$k=1,2,\dotsc$ until convergence}{
      $\sigma_k = \inp{\mat{A}\vec{v}_{k-1}}{\vec{v}_{k-1}}_2$\;
      $\vec{\tilde{v}}_k = {(\mat{A} - \sigma_k \mat{I})}^{-1}
      \vec{v}_{k-1}$\;
      $\vec{v}_k = \vec{\tilde{v}}_k / \norm{\vec{\tilde{v}}_k}_2$\; }
  }
  \caption{Rayleigh quotient iteration}\label{alg:rqi}
\end{algorithm}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
