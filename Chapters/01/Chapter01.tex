\chapter{Introduction}
In this chapter, we introduce the general problem by discussing an
example that naturally arises in different physical applications. We
will then define that basic notion of the Rayleigh quotient and derive
the Rayleigh quotient iteration (or RQI, for short). Finally, we
provide an overview of the historic developments of RQI.%
\section{Motivation}

\section{General problem}
We will now formulate the abstract problem that was derived in the
last section. To that end, we will give a few basics facts and
definitions from linear algebra.
\begin{definition}
  Let $\mat{A} \in \C^{n \times n}$ be a matrix. A scalar
  $\lambda \in \C$ is called \emph{eigenvalue} of $\mat{A}$ if there
  exists a nonzero vector $v \in \C^n$ such that
  \begin{equation}
    \label{eq:eigvalproblem}
    \mat{A}\vec{v} = \lambda \vec{v}\,.
  \end{equation}
  The vector $\vec{v}$ is called an \emph{eigenvector} of $\mat{A}$
  associated with $\lambda$. The tuple $(\lambda, \vec{v})$ is called
  an \emph{eigenpair}. The set of all eigenvalues of $\mat{A}$ is
  called the \emph{spectrum} and is denoted by $\sigma(\mat{A})$.
\end{definition}
In the following proposition we combine some basic facts on
eigenvalues and eigenvectors. The results are shown under the
assumption that $\mat{A} \in \C^{n \times n}$ is a complex Hermitian
matrix, \ie $\mat{A} = \overline{\mat{A}}^\tp$, where the bar denotes
the complex conjugate. If $\mat{A}$ is a real matrix, we have
$\overline{\mat{A}} = \mat{A}$ and thus the following facts hold in
particular for real symmetric matrices.
\begin{proposition}%
  \label{prop:eigval:facts}
  Let $\mat{A} = \overline{\mat{A}}^\tp \in \C^{n \times n}$ be a
  Hermitian matrix. Denote by
  $\lambda_1, \lambda_2, \dotsc, \lambda_n$ the eigenvalues of
  $\mat{A}$ with associated eigenvectors
  $\vec{v}_1, \dotsc, \vec{v}_n$.
  \begin{enumerate}[label=(\roman*)]
  \item All eigenvalues of $\mat{A}$ are real.
  \item There exists an orthonormnal basis of $\C^n$ consisting of
    eigenvectors of $\mat{A}$.
    % $\vec{v}_i$ and $\vec{v}_j$ to two distinct eigenvalues
    % $\lambda_i$ and $\lambda_j$ are orthogonal. Hence, after
    % normalisation, we can choose eigenvectors of $\mat{A}$ that form
    % an orthonormal basis of $\R^n$.
  \item If $\mat{A}$ is non-singular the eigenvalues of $\mat{A}^{-1}$
    are given by $\lambda_1^{-1}, \dotsc, \lambda_n^{-1}$ with
    eigenvectors $\vec{v}_1, \dotsc, \vec{v}_n$.
  \item Let $\sigma \in \R$ an arbitrary scalar. Then the eigenvalues
    of $\mat{A} - \sigma \mat{I}$ are $\lambda_i - \sigma$ with
    eigenvectors $\vec{v}_1, \dotsc, \vec{v}_n$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  The proofs for (i) and (ii) can be found in most standard linear
  algebra literature, for example in~\cite[Lemma and Theorem 5.6.2,
  p.~312]{Fischer2013}.
  \begin{enumerate}
  \item[(iii)] Suppose $\mat{A}$ is invertible and let
    $(\lambda, \vec{v})$ be an eigenpair of $\mat{A}$ (note that since
    $\mat{A}$ is non-singular we have $\lambda \neq 0$). Then
    \begin{gather*}
      \mat{A} \vec{v} = \lambda \vec{v} \quad \Leftrightarrow \quad
      \mat{A}^{-1} \mat{A} \vec{v} = \lambda \mat{A}^{-1} \vec{v}
      \quad \Leftrightarrow \quad \lambda^{-1} \vec{v} = \mat{A}^{-1}
      \vec{v} \,,
    \end{gather*}
    \ie $(\lambda^{-1}, \vec{v})$ is an eigenpair of $\mat{A}^{-1}$.
  \end{enumerate}
  \todo{Finish proof}
\end{proof}
In the following we restrict our attention to the \emph{symmetric
  eigenvalue problem}, \ie we want to find solutions of
equation~\eqref{eq:eigvalproblem} given $\mat{A} \in \R^{n \times n}$
is a real symmetric matrix. The eigenvalues and associated
eigenvectors of $\mat{A}$ are denoted by
$\lambda_1, \dotsc, \lambda_n$ and $\vec{v}_1, \dotsc, \vec{v}_n$,
respectively. We assume that the eigenvalues are normalised \wrt the
Euclidean norm, \ie
\[
  \norm{\vec{v}_i} = \sqrt{ \vec{v}_i^\tp \vec{v}_i } = 1 \quad \text{
    for all } i = 1, \dotsc, n\,.
\]
Note, that due to Proposition~\ref{prop:eigval:facts} (ii) we have
\[
  \inp{\vec{v}_i}{\vec{v}_j} = \vec{v}_i^\tp \vec{v}_j = 0 \quad
  \text{ for } i \neq j\,,
\]
where $\inp{\cdot}{\cdot}$ denotes the Euclidean inner product on
$\R^n$. Since all eigenvalues are real we can label them in increasing
order of magnitude
\[
  \abs{\lambda_1} \le \abs{\lambda_2} \le \dotsc \le \abs{\lambda_n}
  \,.
\]
The eigenvalues $\lambda_1$ and $\lambda_n$ are called \emph{extreme}
eigenvalues. The remaining eigenvalues
$\lambda_2, \dotsc, \lambda_{n-1}$ are called \emph{interior}
eigenvalues. Sometimes we will make the assumption that $\mat{A}$ is
positive definite, \ie that
\[
  x^\tp A x > 0 \quad \text{ for all } x \in \R^n.
\]
Given an eigenvalue $\lambda$ and an associated eigenvector $\vec{v}$
we then have
\[
  \lambda = \vec{v}^\tp \lambda \vec{v} = \vec{v}^\tp \mat{A} \vec{v}
  > 0 \,.
\]
\ie eigenvalues of symmetric positive definite matrices are positive.

\section{Iterative methods for eigenvalue problems}%
\label{sec:iterative_algorithms}
In this section we introduce the most simple iterative methods for
computing eigenpairs of symmetric matrices.
\subsection{Power method}
% Troughout the entire thesis we will follow the notation
% from~\cite{Parlett1998}.
The power method is based on generating the sequence
$\mat{A}^k \vec{v}_0$ where $\vec{v}_0$ is a non-zero unit vector. Of
course, $\mat{A}^k$ does not have to be computed explicitly since
\[
  \mat{A}^k \vec{x} = \mat{A}(\mat{A}(\dotsc\mat{A}(\mat{A}
  \vec{x})\dotsc))\,.
\]
The sequence $\vec{v}_k$ as generated in
Algorithm~\ref{alg:power:method} converges to the eigenvector
associated with the eigenvalue $\lambda_n$ (under the assumption that
$\lambda_n$ is the unique dominant eigenvalue, cf.~\cite[Theorem
4.2.1]{Parlett1998}).
\begin{algorithm}[htbp]
  \DontPrintSemicolon%
  \KwData{Nonzero unit vector $\vec{v}_0$}
  \For{$k = 1,2, \dotsc$ until convergence}{
    $\vec{\tilde{v}}_k \gets \mat{A} \vec{v}_{k-1}$\;
    $\vec{v}_k \gets \vec{\tilde{v}}_k / \norm{\vec{\tilde{v}}_k}$\;
  } 
  \caption{Power method}\label{alg:power:method}
\end{algorithm}
The power method converges linearly with convergence factor
$\abs{\lambda_{n-1}}/\abs{\lambda_n}$.
\subsection{(Shifted) Inverse Iteration}
The \emph{inverse iteration} is the power method applied to
$\mat{A}^{-1}$ (given $\mat{A}$ is non-singular). Due to
Proposition~\ref{prop:eigval:facts} (iii) this will produce a sequence
of vectors $\vec{v}_k$ converging to the eigenvector associated to the
smallest eigenvalue $\lambda_1$. Combining this idea with
Proposition~\ref{prop:eigval:facts} (iv) produces the \emph{shifted
  inverse iteration}.\footnote{Note that some authors call this the
  inverse iteration and do not make a distinction of a shift being
  used or not.} There, the iterates are defined by
\[
  \vec{v}_k = {\left( \mat{A} - \sigma \mat{I} \right)}^{-1}
  \vec{v}_{k-1}\,,
\]
where we have omitted the normalisation step. The scalar
$\sigma \in \R$ is often called the \emph{shift} since it shifts the
eigenvalues of $\mat{A}^{-1}$ by $\sigma$ on the real axis. Of course,
we do not have to compute the inverse explicitly. Instead, before the
loop we can compute the $\mat{LU}$ decomposition of
$\mat{A} - \sigma \mat{I}$ and solve the system
\[
  {\left( \mat{A} - \sigma \mat{I} \right)} \vec{v}_k = \vec{v}_{k-1}
\]
for $\vec{v}_k$ reducing the complexity from $\O(n^3)$ to $\O(n^2)$ at
each step. We summarise the results in Algorithm~\ref{alg:sii}.

\begin{algorithm}[htbp]
  \DontPrintSemicolon%
  \KwData{Nonzero unit vector $\vec{v}_0$, shift $\sigma \in \R$}
  Compute $\mat{LU}$ decomposition $\mat{A} - \sigma \mat{I} = \mat{LU}$\;
  \For{$k = 1,2, \dotsc$ until convergence}{
    Solve $\vec{v}_{k-1} = {\left( \mat{A} - \sigma \mat{I}\right)} \vec{\tilde{v}}_{k}$ for $\vec{\tilde{v}}_{k}$\;
    $\vec{v}_k \gets \vec{\tilde{v}}_k / \norm{\vec{\tilde{v}}_k}$\;
  } 
  \caption{Shifted inverse iteration}\label{alg:sii}
\end{algorithm}

\subsection{Rayleigh Quotient Iteration}

\begin{definition}[Rayleigh quotient]
  Let $\mat{A} \in \R^{n \times n}$. The mapping
  \[
    \mathcal{R}_{\mat{A}} : \R^n \setminus \{\vec{0}\} \rightarrow \R,
    \qquad \vec{x} \mapsto \frac{\vec{x}^\tp \mat{A} \vec{x}}{\vec{x}^\tp
      \vec{x}}
  \]
  is called the \emph{Rayleigh quotient} corresponding to the matrix
  $\mat{A}$.
\end{definition}

\section{History}
In this section we present several publications that contributed towards the Rayleigh Quotient iteration.\todo{Formulierung}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
