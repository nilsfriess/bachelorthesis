\section{Results from numerical linear algebra}
In this section we introduce the definitions and results from linear
algebra that will be needed later. We also introduce the most basic
iterative eigenvalue algorithms.

\subsection{The Symmetric Eigenvalue Problem}%
\label{sec:symmetric:eigenproblem}
In Example~\ref{ex:modelproblem} a problem of the form
\[
  \text{``Find
    $\vec{v} \in \R^n \setminus \{ \vec{0} \},\, \lambda \in \R$, such
    that $ \mat{A}\vec{v} = \lambda \vec{v}$ ''}
\]
arised, where $\mat{A}$ was a real symmetric matrix. This problem is
referred to as the \emph{symmetric eigenvalue problem}. A more general
definition is given in the following.
\begin{definition}
  Let $\mat{A} \in \C^{n \times n}$. A scalar $\lambda \in \C$ is
  called \emph{eigenvalue} of $\mat{A}$ if there exists a nonzero
  vector $\vec{v} \in \C^n$ such that
  \begin{equation}
    \label{eq:eigvalproblem} 
    \mat{A}\vec{v} = \lambda \vec{v}\,.
  \end{equation}
  The vector $\vec{v}$ is called an \emph{eigenvector} of $\mat{A}$
  associated with $\lambda$. The tuple $(\lambda, \vec{v})$ is called
  an \emph{eigenpair}. The set of all eigenvalues of $\mat{A}$ is
  referred to as the \emph{spectrum} and is denoted by
  $\sigma(\mat{A})$. To indicate that eigenvalues belong to a
  particular matrix $\mat{M}$ we sometimes write $\lambda(\mat{M})$.
\end{definition}
Computing eigenpairs is a non-trivial task.
Rewriting~\eqref{eq:eigvalproblem} gives
$\mat{A}\vec{v} - \lambda \vec{v} = \vec{0}$ or
$(\mat{A} - \lambda \mat{I}) \vec{v} = \vec{0}$, where $\mat{I}$ is
the identity matrix. Since $\vec{v}$ cannot be the zero vector, this
equation has a solution if and only if the matrix
$\mat{A} - \lambda \mat{I}$ is singular. Thus, eigenvalues of
$\mat{A}$ are exactly the roots of the \emph{characteristic
  polynomial}
\[
  \chi_{\mat{A}}(t) \coloneqq \det(\mat{A} - t \mat{I}) \,.
\]
This fact, despite being of theoretical importance, cannot be used to
calculate eigenvalues numerically for two reasons. First, the
computation of the coefficients of the polynomial is not
stable~\cite[37]{golub2000eigenvalue}. And even if it was, it is
well-known that even small perturbations in the coefficients of
$\chi_{\mat{A}}(t)$ can lead to devastating errors in the
roots~\cite[cf.][190]{trefethen1997}. Thus, other methods are
necessary to solve~\eqref{eq:eigvalproblem} which gave rise to
iterative algorithms. These methods date back to 1846 when Jacobi
published a pioneering paper on a method to compute eigenvalues of
symmetric matrices~\cite{jacobi1846}. Below we present essential facts
from linear algebra preparing us for discussing such iterative methods
in Section~\ref{sec:iterative:algorithms}.

\begin{remark}[Generalisations of eigenvalue problems]
  The problem stated in Equation~\eqref{eq:eigvalproblem} can be
  generalised in multiple ways. Many problems from physics lead to the
  \emph{generalised eigenvalue problem}
  \begin{equation}
    \label{eq:eigvalproblem:general}
    \mat{A} \vec{v} = \lambda \mat{M} \vec{v}\,.
  \end{equation}
  In our case, we have $\mat{M} = \mat{I}$, the identity matrix. Many
  of the numerical algorithms for solving eigenvalue problems of the
  form~\eqref{eq:eigvalproblem} can be modified to
  solve~\eqref{eq:eigvalproblem:general}; often certain assumptions
  have to be posed on $\mat{M}$ such as positive definiteness.

  Since matrices can be seen as representations of linear operators on
  finite-dimensional vector spaces, we can define eigenvalue problems
  for linear operators on more general spaces, that are possibly of
  infinite dimension. The eigenvectors are then usually called
  \emph{eigenfunctions}. Other generalisations include the
  \emph{quadratic eigenvalue problem}
  \begin{equation*}
    (\lambda^2 \mat{A}_2 + \lambda \mat{A}_1) \vec{v} = \mat{A}_0 \vec{v}\,,
  \end{equation*}
  with matrix coefficients
  $\mat{A}_0, \mat{A}_1, \mat{A}_2 \in \C^{n \times n}$ or more
  general \emph{nonlinear eigenproblems}
  \begin{equation*}
    \mat{Q}(\lambda)\vec{v} = \vec{0}\,,
  \end{equation*}
  where $\mat{Q}(\lambda)$ is a nonlinear matrix-valued function.  In
  this thesis, we only consider problems of the
  form~\eqref{eq:eigvalproblem}.
\end{remark}

In the following proposition we collect some basic facts on
eigenvalues and eigenvectors. The results are shown under the
assumption that $\mat{A} \in \C^{n \times n}$ is a complex Hermitian
matrix, \ie
$\mat{A} = \mat{A}^\herm \coloneqq \overline{\mat{A}}^\tp$, where the
bar denotes the complex conjugate. If $\mat{A}$ is a real matrix, we
have $\overline{\mat{A}} = \mat{A}$ and thus the following facts hold
in particular for real symmetric matrices.
\begin{proposition}%
  \label{prop:eigval:facts}
  Let $\mat{A} = \mat{A}^\herm \in \C^{n \times n}$ be a Hermitian
  matrix. Denote by $\lambda_1, \lambda_2, \dotsc, \lambda_n$ the
  eigenvalues\footnote{Of course, the eigenvalues need not be
    distinct. But since the eigenvalues of $\mat{A}$ are the roots of
    the $n$-degree polynomial $\chi_{\mat{A}}(t)$, when counting these
    roots with their multiplicity, this polynomial has $n$ roots over
    $\C$. Thus, we can label the eigenvalues from $1$ to $n$.} of
  $\mat{A}$ with associated eigenvectors
  $\vec{v}_1, \dotsc, \vec{v}_n$.
  \begin{enumerate}[label=(\roman*)]
  \item All eigenvalues of $\mat{A}$ are real.
  \item There exists an orthonormal basis of $\C^n$ consisting of
    eigenvectors of $\mat{A}$. If $\mat{A}$ is a real symmetric
    matrix, the eigenvectors form an orthonormal basis of $\R^n$.
    % $\vec{v}_i$ and $\vec{v}_j$ to two distinct eigenvalues
    % $\lambda_i$ and $\lambda_j$ are orthogonal. Hence, after
    % normalisation, we can choose eigenvectors of $\mat{A}$ that form
    % an orthonormal basis of $\R^n$.
  \item If $\mat{A}$ is non-singular the eigenvalues of $\mat{A}^{-1}$
    are given by $\lambda_1^{-1}, \dotsc, \lambda_n^{-1}$ with
    eigenvectors $\vec{v}_1, \dotsc, \vec{v}_n$.
  \item Let $\mu \in \R$ an arbitrary scalar. The eigenvalues of
    $\mat{A} - \mu \mat{I}$ are $\lambda_i - \mu$ with eigenvectors
    $\vec{v}_1, \dotsc, \vec{v}_n$.
  \end{enumerate}
\end{proposition}
\begin{proof}
  Both (i) and (ii) are well-known results from linear algebra and the
  proofs can be found in most standard literature (see for
  example~\cite[Theorem 18 and Corollary, p.~314]{hoffmanlinalg}).
  \begin{enumerate}
  \item[(iii)] Suppose $\mat{A}$ is invertible and let
    $(\lambda, \vec{v})$ be an eigenpair of $\mat{A}$ (note that since
    $\mat{A}$ is non-singular we have $\lambda \neq 0$). Then
    \begin{gather*}
      \mat{A} \vec{v} = \lambda \vec{v} \quad \Leftrightarrow \quad
      \mat{A}^{-1} \mat{A} \vec{v} = \lambda \mat{A}^{-1} \vec{v}
      \quad \Leftrightarrow \quad \lambda^{-1} \vec{v} = \mat{A}^{-1}
      \vec{v} \,,
    \end{gather*}
    hence $(\lambda^{-1}, \vec{v})$ is an eigenpair of $\mat{A}^{-1}$.

  \item[(iv)] For $\mu \in \R$ and $(\lambda, \vec{v})$ an eigenpair
    we have
    \begin{gather*}
      \mat{A}\vec{v} = \lambda \vec{v} \quad \Leftrightarrow \quad
      \mat{A}\vec{v} - \mu \vec{v} = \lambda \vec{v} - \mu \vec{v}
      \quad \Leftrightarrow \quad (\mat{A} - \mu \mat{I}) \vec{v} =
      (\lambda - \mu) \vec{v} \,,
    \end{gather*}
    hence $(\lambda - \mu, \vec{v})$ is an eigenpair of
    $\mat{A} - \mu \mat{I}$.
  \end{enumerate}
\end{proof}

In the following, we restrict our attention to the \emph{symmetric
  eigenvalue problem}, \ie we want to find solutions of
equation~\eqref{eq:eigvalproblem} assuming $\mat{A}$ is a real
symmetric matrix and thus, if not stated otherwise, for the remainder
of the thesis $\mat{A}$ denotes a matrix of this type. The (real)
eigenvalues are denoted by $\lambda_j(\mat{A}) = \lambda_j$ with
corresponding (real) eigenvectors $\vec{v}_j$. Since any scalar
multiple of an eigenvector is also an eigenvector, we assume that they
are normalised \wrt the Euclidean norm, so that
\begin{equation*}
  \norm{\vec{v}_i} \coloneqq \norm{\vec{v}_i}_2
  \coloneqq \sqrt{\vec{v}_i^\tp \vec{v}_i }
  = 1 \quad \text{ for all } i = 1, \dotsc, n\,.
\end{equation*}
Due to Proposition~\ref{prop:eigval:facts} (ii) we have
\begin{equation*}
  \inp{\vec{v}_i}{\vec{v}_j} = \vec{v}_i^\tp \vec{v}_j = 0 \quad
  \text{ for } i \neq j\,,
\end{equation*}
where $\inp{\cdot}{\cdot}$ denotes the Euclidean inner product on
$\R^n$. Since all eigenvalues are real we can label them in increasing
order of magnitude
\begin{equation*}
  \abs{\lambda_1} \le \abs{\lambda_2} \le \dotsc \le \abs{\lambda_n}
  \,.
\end{equation*}
The eigenvalues $\lambda_1$ and $\lambda_n$ are called \emph{extreme}
eigenvalues. The remaining eigenvalues
$\lambda_2, \dotsc, \lambda_{n-1}$ are called \emph{interior}
eigenvalues.

In addition to the assumption that the matrix we work with is real and
symmetric, we are mainly interested in cases in which the target
eigenvalue and its neighbours are very close. We usually do not have
any a priori knowledge about their location. Also, the matrices are
assumed to be large such that the computation of the complete set of
eigenpairs is too expensive. However, we assume a good approximation
of the target eigenvector is available. As we will see, traditional
methods do not perform well when the gap between the wanted eigenvalue
and adjacent eigenvalues is too small.

% Sometimes we will make the assumption that $\mat{A}$ is positive
% definite, \ie that
% \[
%   x^\tp A x > 0 \quad \text{ for all } x \in \R^n \setminus \{
%   \vec{0} \}.
% \]
% Given an eigenvalue $\lambda$ and an associated normalised
% eigenvector $\vec{v}$ we then have
% \[
%   \lambda = \vec{v}^\tp \lambda \vec{v} = \vec{v}^\tp \mat{A}
%   \vec{v} > 0 \,,
% \]
% \ie eigenvalues of symmetric positive definite matrices are
% positive.

\subsection{Iterative methods for eigenvalue problems}%
\label{sec:iterative:algorithms}
With the necessary facts from linear algebra at hand we can introduce
some simple iterative methods for computing eigenpairs of symmetric
matrices. We are always interested in how fast these methods produce
good approximations of eigenvectors or eigenvalues (or both). The
following definition provides us with a notion of the speed at which a
sequence converges to its limit.

\begin{definition}[Order of Convergence]
  Let ${(\vec{x}^{(k)})}_{k \in \N}$ be a sequence in $\C^n$ that
  converges to $\vec{z} \in \C^n$.
  \begin{enumerate}[label=(\roman*)]
  \item The sequence is said to converge \emph{linearly} to $\vec{z}$,
    if there exists a constant $0 < \rho < 1$ such that
    \begin{equation*}
      \lim_{k \rightarrow \infty} \frac{
        \norm{\vec{x}^{(k+1)} - \vec{z}}
      }{
        \norm{\vec{x}^{(k)} - \vec{z}}
      } < \rho\,,
    \end{equation*}
    where $\rho$ is called the \emph{rate of convergence}.
  \item The sequence \emph{converges with order $q$ to $\vec{z}$} for
    $q > 1$ if
    \begin{equation*}
      \lim_{k \rightarrow \infty} \frac{
        \norm{\vec{x}^{(k+1)} - \vec{z}}
      }{
        \norm{\vec{x}^{(k)} - \vec{z}}^q
      } < M\,,
    \end{equation*}
    for some $M > 0$. In particular, convergence with order
    \begin{itemize}
    \item $q = 2$ is called \emph{quadratic convergence},
    \item $q = 3$ is called \emph{cubic convergence}
    \end{itemize}
    etc.
  \end{enumerate}
\end{definition}

In some cases, in particular for sequences that approximate
eigenvectors, the convergence behaviour is best studied in terms of
the \emph{error angle} between $\vec{x}^{(k)}$ and $\vec{z}$.

\begin{definition}[Angle]
  The \emph{angle} between two vectors
  $\vec{x}, \vec{y} \in \C^n \setminus \{ \vec{0} \}$ is defined as
  \begin{equation*}
    \angle(\vec{x}, \vec{y}) = \arccos \frac{
      \abs{\langle \vec{x}, \vec{y} \rangle}
    }{
      \norm{\vec{x}} \norm{\vec{y}}
    }\,.
  \end{equation*}
  Often, the following identities are convenient
  \begin{gather*}
    \sin \angle(\vec{x}, \vec{y}) \coloneqq \sqrt{1 - \cos^2
      \angle(\vec{x}, \vec{y})}\,, \qquad \tan \angle(\vec{x},
    \vec{y}) \coloneqq \frac{ \sin \angle(\vec{x}, \vec{y}) }{ \cos
      \angle(\vec{x}, \vec{y}) }\,.
  \end{gather*}
\end{definition}

To see why also the angle can be used to measure the convergence
speed, suppose $\vec{x}^{(k)}$ converges to the unit vector
$\vec{z}$. Let $\vec{u}^{(k)}$ be the unit vector that lies in the
span of $\vec{x}^{(k)}$ and $\vec{z}$ and is orthogonal to $\vec{z}$
and denote by $\phi^{(k)} = \angle(\vec{x}^{(k)}, \vec{z})$ the error
angle between the current vector iterate and the limit. Now, write the
vector iterate $\vec{x}^{(k)}$ as
\begin{equation*}
  \vec{x}^{(k)} = \vec{z} \cos \phi^{(k)} + \vec{u}^{(k)} \sin \phi^{(k)}\,.
\end{equation*}
We temporarily drop the superscripts and write
$\vec{x} = \vec{x}^{(k)}$, $\vec{u} = \vec{u}^{(k)}$ and
$\phi = \phi^{(k)}$. Then, using the identities
$\sin^2(\phi / 2) = \frac{1 - \cos \phi}{2}$,
$\sin^2 \phi + \cos^2 \phi = 1$ and the Pythagorean theorem we obtain
\begin{align*}
  \norm{\vec{x} - \vec{z}}^2 &= \norm{\vec{z} \cos \phi + \vec{u} \sin \phi - \vec{z}}^2 \\
                             &= \norm{\vec{z}(\cos \phi - 1)}^2 + \norm{\vec{u} \sin \phi}^2 \\
                             &= {(\cos \phi - 1)}^2 + 1 - \cos^2 \phi \\
                             &= 2(1 - \cos \phi) = 4 \sin^2( \phi / 2)\,.
\end{align*}
Thus, convergence orders \wrt the norm imply the same convergence
orders in terms of the error angles and vice verca. Note that we did
assume convergence of the sequence.

\subsubsection{Power method}
The \emph{power method} is one of the oldest iterative methods for
computing eigenvectors. It is based on generating the sequence
$\vec{x}^{(k)} \coloneqq \mat{A}^k \vec{x}^{(0)}$ where
$\vec{x}^{(0)}$ is a non-zero unit vector. Of course, $\mat{A}^k$ does
not have to be computed explicitly at each step since
\[
  \mat{A}^k \vec{x} = \mat{A}(\mat{A}(\dotsc\mat{A}(\mat{A}
  \vec{x})\dotsc))\,.
\]
To prevent underflow and overflow errors, $\vec{x}^{(k)}$ is
normalised at each step. In Algorithm~\ref{alg:power:method} we
normalise by ensuring that the largest component of the current
approximation is equal to one. Of course, other norms can be used. The
sequence $\vec{x}^{(k)}$ converges to the eigenvector associated with
the eigenvalue $\lambda_n$ under the assumptions that $\lambda_n$ is
dominant (\ie $\abs{\lambda_n}$ is strictly greater than
$\abs{\lambda_{n-1}}$) and that the starting vector $\vec{x}^{(0)}$
has a non-vanishing component in the direction of $\vec{v}_n$. The
advantage of normalising \wrt the maximum norm is that the largest
component of $\abs*{\mat{A}\vec{x}^{(k-1)}}$ converges to the
eigenvalue $\lambda_n$. Regardless of the normalisation chosen, the
method converges linearly with convergence rate
\begin{equation}
  \label{eq:convergence:power}
  \rho = \frac{\abs{\lambda_{n-1}}}{\abs{\lambda_n}}\,.
\end{equation}
Thus, the method can be very slow if the distance between the
eigenvalues $\lambda_n$ and $\lambda_{n-1}$ is very small. For a
detailed convergence proof, see~\cite[86\psq]{saad2011}.

\begin{algorithm}[htpb]
  \DontPrintSemicolon
  \Begin{ Choose nonzero initial vector $\vec{x}^{(0)}$\;
    \For{$k = 1,2,\dotsc$ until convergence}{
      $\displaystyle \vec{x}^{(k)} =
      \frac{1}{\alpha^{(k)}}\mat{A}\vec{x}^{(k-1)}$\; \tcc{$\alpha^{(k)}$ is the component of $\mat{A}\vec{x}^{(k-1)}$ with the maximum modulus}
    }
  }
  \caption{Power method}\label{alg:power:method}
\end{algorithm}

Besides the possible slow convergence rate, the power method will
always converge to an eigenvector associated with the dominant
eigenvalue $\lambda_n$. In many applications, however, one already has
a good approximation of another eigenvalue and wants to compute an
eigenvector it belongs to. The following method allows for such
computations.

\subsubsection{(Shifted) Inverse Iteration}
The \emph{inverse iteration} is the power method applied to
$\mat{A}^{-1}$ (provided that the inverse exists). Due to
Proposition~\ref{prop:eigval:facts} (iii) this will produce a sequence
of vectors $\vec{x}^{(k)}$ converging to the eigenvector associated to
the eigenvalue that is smallest in modulus $\lambda_1$. Combining this
idea with Proposition~\ref{prop:eigval:facts} (iv) yields the
\emph{shifted inverse iteration}. There, the iterates are defined by
\[
  \vec{x}^{(k)} = \beta {\left( \mat{A} - \sigma \mat{I} \right)}^{-1}
  \vec{x}^{(k-1)}\,,
\]
where $\beta$ is responsible for normalising $\vec{x}^{(k)}$. The
smallest eigenvalue in modulus of the shifted matrix
$\mat{A} - \sigma \mat{I}$ is the eigenvalue of $\mat{A}$ that is
closest to $\sigma$. Hence, this method converges to an eigenvector
associated with this eigenvalue. Of course, the inverse need not be
computed explicitly. Instead, before the loop we can compute the LU
decomposition of $\mat{A} - \sigma \mat{I}$ (or any other
factorisation, if applicable) and solve the system
${\left( \mat{A} - \sigma \mat{I} \right)} \vec{x}^{(k)} =
\vec{x}^{(k-1)}$ for $\vec{x}^{(k)}$. At each step then, only one
backward and one forward substitution is required, reducing the
complexity from $\O(n^3)$ to $\O(n^2)$. We summarise the results in
Algorithm~\ref{alg:sii} (there, we normalise \wrt the Euclidean norm).

\begin{algorithm}[htbp]
  \DontPrintSemicolon%
  \KwData{Nonzero unit vector $\vec{x}^{(0)}$, shift $\sigma \in \R$}
  Compute $\mat{LU}$ decomposition $\mat{A} - \sigma \mat{I} = \mat{LU}$\;
  \For{$k = 1,2, \dotsc$ until convergence}{
    Solve ${\left( \mat{A} - \sigma \mat{I}\right)} \vec{\tilde{x}}^{(k)} = \vec{x}^{(k-1)}$ for $\vec{\tilde{x}}^{(k)}$\;
    $\vec{x}^{(k)} \gets \vec{\tilde{x}}^{(k)} / \norm*{\vec{\tilde{x}}^{(k)}}$\;
  } 
  \caption{Shifted inverse iteration}\label{alg:sii}
\end{algorithm}
Since this is essentially the power method (applied to the inverse of
$\mat{A} - \sigma \mat{I}$) this algorithm still converges
linearly. However, if we denote by $\mu_1$ the eigenvalue that is
closest to the shift $\sigma$ and by $\mu_2$ the one that is the next
closest one, the eigenvalue of largest modulus of
${(\mat{A} - \sigma \mat{I})}^{-1}$ is $1 / (\mu_1 - \sigma)$
and~\eqref{eq:convergence:power} suggests that the convergence rate is
\[
  \rho = \frac{\abs{\mu_1 - \sigma}}{\abs{\mu_2 - \sigma}} \,.
\]
Therefore, the method is often used to compute an eigenvector of
$\mat{A}$ if a good approximation of the corresponding eigenvalue is
already available. Note, however, that a shift which is very close to
an eigenvalue produces a very ill-conditioned linear system. In the
case when $\sigma$ is exactly an eigenvalue the system is even
singular and one might expect inverse iteration to fail in these cases
since, in general, it is impossible to solve ill-conditioned systems
accurately. Despite this seemingly sincere problem, in practice it was
observed that the method produces good approximations even in these
cases. According to Parlett~\cite[84\psq]{Parlett1998}, it was
Wilkinson who elucidated why the ill-conditioning is not a problem in
most cases. Suppose $\sigma \approx \lambda$ where $\lambda$ is an
eigenvalue of $\mat{A}$ with corresponding eigenvector
$\vec{v}$. Wilkinson illustrated that although $\tilde{\vec{x}}^{(k)}$
may be far from $\vec{v}$, the normalised solution
$\vec{x}^{(k)} = \tilde{\vec{x}}^{(k)} /
\norm*{\tilde{\vec{x}}^{(k)}}$ will not be far from $\vec{v}$, when
the system is solved backwards-stably, for more details
see~\cite[621--630]{wilkinson},~\cite[68--71]{Parlett1998}
and~\cite{petersWilkinson}. This will become important again later
when we discuss Rayleigh Quotient Iteration. There, the system that is
solved gets increasingly ill-conditioned at each step but for the same
reason as above, in practice this poses no problem.

Note that we did not specify the ``until convergence'' criteria in
neither of the algorithms above. We postpone this discussion until
Section~\ref{sec:rqi:rq}.

At each step in the shifted inverse iteration, better approximations
for the target eigenvector are computed. One could try to use these
approximations to replace occasionally the shift by an approximation
of the corresponding eigenvalue. There are different techniques to
obtain such estimates, \eg the \emph{Wielandt Shifted Inverse
  Iteration} or the \emph{Rayleigh Quotient Iteration}, the latter of
which is rigorously studied in the next chapter. For further
discussion on the variants and developments of these so called
\emph{shift and invert} techniques see, \eg~\cite{ipsenhistory,
  golub2000eigenvalue, tapia2018}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
 