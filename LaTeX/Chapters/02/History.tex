\section{History and Recent Developments}
Now that we have defined the Rayleigh Quotient and Rayleigh Quotient
Iteration we give an overview of the historic developments of RQI. We
also discuss recent contributions that are relevant to this thesis.
Some of the results, mainly the ones that are concerned with the
convergence of RQI, are discussed in more detail in
Section~\ref{sec:convergence}.

\subsection{Chronology of Rayleigh Quotient Iteration}
It took about 60 years from the first mention of what is now called
the Rayleigh quotient until RQI was fully defined as it is given in
Algorithm~\ref{alg:rqi}. In this section we give an overview of some
important milestones within this 60 years. Large parts of this
overview are based on Section 4 of~\cite{tapia2018}.

\paragraph{1894 --- \textsc{Lord Rayleigh}} In the second edition of
his book titled ``The Theory of Sound'' John William Strutt, 3rd Baron
Rayleigh~\cite[110]{rayleigh}, proposed the following iteration for
improving an approximate eigenvector $\vec{x}^{(0)}$:
\begin{equation}
  \label{eq:rayleigh:iteration}
  \text{Solve}\quad (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{I})\, \vec{x}^{(i+1)} = \vec{e}_1\,,
\end{equation}
where $\vec{e}_1$ denotes the first natural coordinate vector, \ie the
first column of the identity matrix and $\vec{x}^{(i)}$ and
$\vec{x}^{(i+1)}$ denote the current and next iterate,
respectively. In fact, Lord Rayleigh considered the generalised
eigenvalue problem and so in his text, the iteration reads
\begin{equation*}
  \text{Solve}\quad (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{M})\, \vec{x}^{(i+1)} = \vec{e}_1\,.
\end{equation*}

\paragraph{1949 --- \textsc{Kohn}}
In a letter to the editor Walter Kohn~\cite{kohn} suggests the
following iteration
\begin{equation*}
  \text{Solve}\quad (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{I}) \vec{x}^{(i+1)} = \vec{e}_k\,,
\end{equation*}
where $\vec{e}_k$ is \emph{any} of the natural coordinate
vectors. Without a rigorous proof Kohn argues that
$\rq_{\mat{A}}(\vec{x}^{(i)})$ converges quadratically to an
eigenvalue of $\mat{A}$ (provided that $\vec{x}^{(0)}$ is sufficiently
close to an eigenvector of $\mat{A}$). Despite the similarity
to~\eqref{eq:rayleigh:iteration}, Kohn does not mention Lord
Rayleigh's method and it is not known whether or not he was aware of
it.

\paragraph{1951 --- \textsc{Crandall}}
In a text communicated to the Royal society of London Stephen
Crandall~\cite{crandall} suggests
\begin{equation}
  \label{eq:unnormalised:rqi}
  \text{Solve}\quad (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{I}) \vec{x}^{(i+1)} = \vec{x}^{(i)}\,.
\end{equation}
Actually, Crandall also considered the generalised eigenproblem but
for our comparative purposes it is sufficient to consider the case
$\mat{M} = \mat{I}$. Note, that this algorithm is RQI without the
normalisation step. Based on the assumption that the sequence of
vectors $\vec{x}^{(k)}$ converges, Crandall establishes cubic
convergence in this sequence and the approximate eigenvalue
sequence. To see why the assumption is wrong, we assume the contrary,
\ie suppose that the sequence converges to an eigenvector, say
$\vec{v}_k$.  From~\eqref{eq:unnormalised:rqi} we have
\begin{equation*}
  \quad \mat{A}\vec{v}_k - \rq_{\mat{A}}(\vec{v}_k) \vec{v}_k = \vec{v}_k
  \qquad \Leftrightarrow \qquad
  \mat{A}\vec{v}_k = (1 + \rq_{\mat{A}}(\vec{v}_k)) \vec{v}_k\,,
\end{equation*}
and so $\vec{v}_k$ is an eigenvector of $\mat{A}$ with corresponding
eigenvalue $1 + \rq_{\mat{A}}(\vec{v}_k)$. Since we know that for any
eigenvector, the value of $\rq_{\mat{A}}(\vec{v}_k)$ is the eigenvalue
it belongs to, this implies
\begin{equation*}
  \rq_{\mat{A}}(\vec{v}_k) = 1 + \rq_{\mat{A}}(\vec{v}_k)
\end{equation*}
which is a contradiction.

Also, Crandall establishes what is usually referred to as $r$-cubic
convergence. This is a weaker notion of convergence than the one
defined earlier, which is called $q$-cubic convergence
(see~\cite[Appendix~A.1]{tapia2018} for a detailed definition of these
notions of convergence).

\paragraph{1957 -- 59 --- \textsc{Ostrowski}}
Alexander Ostrowski published a series of six papers titled ``On the
Convergence of the Rayleigh Quotient Iteration for the Computation of
the Characteristic Roots and
Vectors. I-VI''~\cite{ostrowskiI,ostrowskiII,ostrowskiIII,ostrowskiIV,ostrowskiV,ostrowskiVI}. We
mention the titles here, as they represent the first mention of the
term \emph{Rayleigh Quotient Iteration}.

In the first paper the author suggests the iteration
\begin{equation}
  \label{eq:rqi:ostrowski:first}
  \text{Solve}\quad (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{I}) \vec{x}^{(i+1)} = \bm{\eta}\,,\quad \bm{\eta} \neq \vec{0}\,.
\end{equation}
He rigorously establishes a \emph{quadratic} convergence rate for the
sequence of Rayleigh Quotients $\rq_{\mat{A}}(\vec{x}^{(i)})$. He then
refers to a paper of Wielandt~\cite{wielandt} and his
\emph{fractional} or \emph{broken iteration} (German: \emph{gebrochene
  Iteration}). Inspired by Wielandt`s method he suggests replacing the
fixed vector $\bm{\eta}$ in the right hand side
of~\eqref{eq:rqi:ostrowski:first} to the solution from the previous
step
\begin{equation}
  \label{eq:rqi:unnormalised:ostrowski}
  (\mat{A} - \rq_{\mat{A}}(\vec{x}^{(i)}) \mat{I}) \vec{x}^{(i+1)} = \vec{x}^{(i)}\,,
\end{equation}
starting with an arbitrary non-zero vector $\vec{x}^{(0)}$. He then
gives a rigorous proof of the local \emph{cubic} convergence of the
sequence of Rayleigh quotients
$\mu_i \coloneqq \rq_{\mat{A}}(\vec{x}^{(i)})$, \ie
\begin{equation}
  \label{eq:rqi:cubic}
  \frac{
    \mu_{i+1} - \lambda
  }{
    {(\mu_i - \lambda)}^3
  }
  \longrightarrow \gamma \quad \text{ as } i \rightarrow \infty\,,
\end{equation}
where $\lambda$ is an eigenvalue of $\mat{A}$ and $\gamma$ is a
positive constant. Local convergence here means that $\vec{x}^{(0)}$
is assumed to be near the eigenvector corresponding to $\lambda$.

Note that~\eqref{eq:rqi:unnormalised:ostrowski} is the same algorithm
previously proposed by Crandall given in
Equation~\eqref{eq:unnormalised:rqi}. Ostrowski was not aware of
Crandall's method; however, while the first paper was in press the
following note was added:

\begin{quotation}
  ``Professor G. Forsythe has directed my attention to a paper by
  S. H. Crandall [\,\ldots]. In particular, Professor Crandall
  establishes the \emph{cubic character} of convergence of $\xi_x$ in
  the rule (28), (29). However he does not arrive at our asymptotic
  formula (46), which is the principal result of our
  paper.''~\cite[241]{ostrowskiI}.\footnote{Here, $\xi_x$ denotes the
    $x$-th iterate of the approximate eigenvector, \ie in our notation
    $\vec{x}^{(x)}$. The rule (28), (29) in Ostrowski's paper
    corresponds to our equation~\eqref{eq:rqi:unnormalised:ostrowski}
    and the asymptotic formula (46) he references is given
    in~\eqref{eq:rqi:cubic}.}
\end{quotation}

In the beginning of the second paper~\cite{ostrowskiII} Ostrowski
discusses this in more detail and remarks that Crandall proofed the
$r$-cubic convergence of the sequence of eigenvalue iterates, while he
showed $q$-cubic convergence.

More importantly, he also points out in ยง21 of the text that in order
to assure convergence in the vector iterates (and not just the
Rayleigh Quotients) one needs to normalise the vectors. With this
small yet important modification of Crandall's algorithm he fully
defined RQI as it is known today.

The third paper~\cite{ostrowskiIII} of the series addresses the
non-symmetric case for which Ostrowski is also able to define a method
that attains local cubic convergence. This method uses a generalised
notion of the Rayleigh quotient and comes at the expense of solving
two linear systems at each step instead of one. The remaining papers
are also mainly concerned with the non-symmetric and non-Hermitian
case and are thus not of particular interest for this thesis.

This concludes the overview of the development of RQI from Lord
Rayleigh's iteration that lacks the changing right hand side and the
normalisation of the vector iterate to the complete definition of RQI
by Ostrowski.

\subsection{Further Developments and Recent
  Contributions}\label{sec:rqi:recent}
We now give an overview of the developments and contributions after
the introduction of RQI in 1958. We also look at some recent
contributions.

After Ostrowski's proof of the local cubic convergence, it took
another ten years until a first important result concerning the
\emph{global} convergence behaviour of RQI was presented. In 1968
Parlett and Kahan~\cite{parlettkahan} showed that RQI applied to
symmetric matrices converges for almost all starting vectors and in
1974 Parlett~\cite{parlettrqi} generalised the result for the case
when $\mat{A}$ is normal. A slightly more concise version of the proof
has been again published in Parlett's book~\cite{Parlett1998}.

Of course, not only is it of interest \emph{that} the method converges
but also \emph{which} eigenpair it converges to. The following example
demonstrates that the convergence behaviour of RQI might sometimes be
unexpected.
\begin{example}
  This example is based on an example
  from~\cite[254]{pantazisszyld}. Let $\mat{A} = \diag(1,\ 2,\ 4)$
  with eigenvalues $\lambda_1=1,\ \lambda_2=2,\ \lambda_3=4$ and
  corresponding eigenvectors $\vec{e}_i$, the columns of the identity
  matrix.
  \begin{enumerate}[label=(\alph*)]
  \item First consider RQI started with the vector
    \begin{equation*}
      \vec{x}^{(0)} =
      \begin{pmatrix}
        0.8163392507169525\\
        -0.0004821161298470036 \\
        0.5775725022046341
      \end{pmatrix}\,.
    \end{equation*}
    This produces the eigenpair $(1, \vec{e}_1)$ although
    \begin{equation*}
      \rq_{\mat{A}}(\vec{x}^{(0)}) = 2.000770218344729\,.
    \end{equation*}
    Note, however, that computing the angle between $\vec{x}^{(0)}$
    and the eigenvectors gives
    \begin{equation*}
      \angle(\vec{x}^{(0)}, \vec{e}_1) \approx 0.61575\,, \quad
      \angle(\vec{x}^{(0)}, \vec{e}_2) \approx 1.57128\,, \quad
      \angle(\vec{x}^{(0)}, \vec{e}_3) \approx 0.95504\,,
    \end{equation*}
    that is, the angle between the initial vector and the resulting
    eigenvector is smallest.
  \item Now take
    \begin{equation*}
      \vec{x}^{(0)} = {\left(
          0.74278,\ 0.55709,\ 0.37139
        \right)}^\tp
    \end{equation*}
    the angles are
    \begin{equation*}
      \angle(\vec{x}^{(0)}, \vec{e}_1) \approx 0.73358\,, \quad
      \angle(\vec{x}^{(0)}, \vec{e}_2) \approx 0.97992\,, \quad
      \angle(\vec{x}^{(0)}, \vec{e}_3) \approx 1.19029\,
    \end{equation*}
    and the initial shift is
    $\rq_{\mat{A}}(\vec{x}^{(0)}) \approx 1.7241$. In this case, RQI
    converges to the eigenpair $(2, \vec{e}_2)$, \ie not the
    eigenvector that makes the smallest angle with $\vec{x}^{(0)}$ but
    rather the one that corresponds to the eigenvalue to which the
    initial shift is closest.
  \end{enumerate}
\end{example}
This example shows that there seems to be no obvious way how the
computed eigenpair depends on $\vec{x}^{(0)}$ or
$\rq_{\mat{A}}(\vec{x}^{(0)})$, and indeed the characterisation of the
global convergence behaviour of RQI is not straightforward. In
contrast, the local convergence behaviour is better
understood. Ostrowski~\cite{ostrowskiI} defined explicit estimates of
\emph{convergence neighbourhoods} for the shift, \ie intervals around
an eigenvalue in which convergence to this eigenvalue is
assured. However, these intervals depend on quantities that are not
known beforehand so they are of little practical use.

Efforts have been made to obtain local convergence regions with as
little knowledge about the spectrum as possible. For example, Beattie
and Fox~\cite{beattiefox} derive conditions under which convergence is
assured to be in a given interval assuming the number of eigenvalues
contained in the interval is known. Recently, Rommes~\cite{rommes}
derived sharper bounds for local convergence neighbourhoods. He
compares RQI to a related algorithm, called the Dominant Pole
Algorithm (DPA). Further, he observes that RQI does not take much
advantage of the information in the initial vector $\vec{x}^{(0)}$. In
other words, even if the initial vector is a very good approximation
of the wanted eigenvector, RQI might fail. We will come back to this
observation in the next chapter.

Szyld~\cite{szyld} suggests combining shifted inverse iteration and
RQI to ensure that the computed eigenvalue lies in a given
interval. He obtains criteria to switch back and forth between the two
algorithms to benefit from their respective advantages (inverse
iteration is guaranteed to converge to the nearest eigenvalue but
convergence is merely linear; RQI possesses local cubic convergence
but the global convergence behaviour is possibly erratic).

The task of identifying global convergence regions in terms of the
initial vector seems to be more difficult. Parlett noted in 1980 that
`[t]here appears to be no simple description of how $\vec{v}$ depends
on $\vec{x}^{(0)}$''~\cite[82]{Parlett1998} and it looks as if this
statement still holds true today. Pantazis and
Szyld~\cite{pantazisszyld} and Batterson and
Smillie~\cite{battersonsmillie} studied \emph{basins of attraction},
\ie regions in the unit sphere from which RQI will converge to a
specific eigenvector. They did, however, only consider the
three-dimensional case.

Besides the unpredictability of the outcome of RQI another major
drawback is its high cost. At every iteration a linear system has to
be solved and since the system changes at each step one cannot
factorise the matrix beforehand as was the case in inverse
iteration. There are several obvious possibilities to reduce the cost
such as to change the shift only occasionally. Sometimes, inverse
iteration is run a fixed number of steps before changing the shift to
the Rayleigh Quotient and Szyld's paper~\cite{szyld} that was
mentioned above can also be interpreted as a method that reduces the
computational cost for large problems. Another approach that was first
studied for inverse iteration is to solve the linear system itself
iteratively leading to an \emph{inexact shift-and-invert method}. The
use of iterative inner solvers in RQI (for Hermitian matrices) was
studied by Simoncini and Eld\'{e}n in~\cite{simoncini2002rqi} and
Notay in~\cite{notay2003rqi}. They both analyse how the convergence of
RQI is affected by solving the linear systems only approximately. This
means at each iteration an approximate solution $\vec{y}^{(k)}$ to
\begin{equation*}
  \left(
    \mat{A} - \mu^{(k)}\mat{I}
  \right)
  \vec{y}^{(k)} = \vec{x}^{(k)}
\end{equation*}
is sought that satisfies
\begin{equation*}
  \norm{
    \left(
      \mat{A} - \mu^{(k)}\mat{I}
    \right)
    \vec{y}^{(k)} - \vec{x}^{(k)}
  } \le \tau^{(k)}\,,
\end{equation*}
where $\tau^{(k)}$ is a tolerance that might change at each
step. In~\cite{simoncini2002rqi} the authors also show the equivalence
of inexact RQI and another iterative eigenvalue method called the
\emph{Jacobi-Davidson method}. This result is extended to the
non-Hermitian case by Freitag and Spence in~\cite{freitag2008rqi}.
There, it is also studied how different preconditioners for the linear
system can be used to improve the outer convergence. We will briefly
come back to the idea of solving the inner system inexactly when
discussing our new method in the next chapter.
 
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
  