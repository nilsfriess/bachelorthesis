\section{The Rayleigh Quotient}\label{sec:rqi:rq}
In Chapter~\ref{chapter:intro} we briefly introduced some simple
iterative eigenvalue methods. In essence, RQI is shifted inverse
iteration where the shift is replaced by the \emph{Rayleigh Quotient}
at each step.

\begin{definition}[Rayleigh Quotient]
  Let $\mat{A} \in \C^{n \times n}$. The mapping
  \begin{equation*}
    \rq_{\mat{A}} : \C^n \setminus \{\vec{0}\} \rightarrow \C, \qquad
    \vec{x} \mapsto \frac{\vec{x}^\herm \mat{A} \vec{x}}{\vec{x}^\herm \vec{x}}
  \end{equation*}
  is called the \emph{Rayleigh Quotient} corresponding to the matrix
  $\mat{A}$.\footnote{Other notations that are popular in the
    literature include $R_{\mat{A}}(\vec{x})$, $R(\mat{A}, \vec{x})$,
    $r_{\mat{A}}(\vec{x})$, $\sigma_{\mat{A}}(\vec{x})$ or
    $\uprho_{\mat{A}}(\vec{x})$.}
\end{definition}
% For real vectors, we have
% \begin{equation*}
%   \rq_{\mat{A}}(\vec{x}) = \frac{\vec{x}^\tp \mat{A} \vec{x}}{\vec{x}^\tp \vec{x}}\,.
% \end{equation*}

We begin by discussing some basic facts.
\begin{lemma}%
  \label{lem:rq:properties}
  Let $\vec{x}\in \C^n \setminus \{ \vec{0} \}$,
  $0 \neq \alpha, \beta \in \C$ and $\mat{A} \in \C^{n \times n}$.
  \begin{enumerate}[label=(\roman*)]
  \item If $(\lambda, \vec{v})$ is an eigenpair of $\mat{A}$, then
    $\rq_{\mat{A}}(\vec{v}) = \lambda$.
  \item
    $\rq_{\beta \mat{A}}(\alpha \vec{x}) = \beta
    \rq_{\mat{A}}(\vec{x})$ \hfill (Homogeneity)

  \item
    $\rq_{\mat{A} - \alpha \mat{I}}(\vec{x}) = \rq_{\mat{A}}(\vec{x})
    - \alpha$ \hfill (Translation invariance)
  \end{enumerate}
\end{lemma}

\begin{proof}
  \begin{enumerate}[label=(\roman*)]
  \item We can write the Rayleigh Quotient as
    \begin{equation}
      \label{eq:rq:innerprod}
      \rq_{\mat{A}}(\vec{x}) = \frac{
        \langle \vec{x}, \mat{A} \vec{x}\rangle
      }{
        \langle \vec{x}, \vec{x} \rangle
      }\,,
    \end{equation}
    where $\langle \vec{x}, \vec{y} \rangle = \vec{x}^\herm \vec{y}$
    denotes the Euclidean inner product on $\C^n$. Due to the
    linearity in the second argument we obtain
    \begin{equation*}
      \rq_{\mat{A}}(\vec{v}) = \frac{
        \langle \vec{v}, \mat{A} \vec{v}\rangle
      }{
        \langle \vec{v}, \vec{v} \rangle
      }
      =
      \frac{
        \langle \vec{v}, \lambda \vec{v}\rangle
      }{
        \langle \vec{v}, \vec{v} \rangle
      }
      =
      \lambda \frac{
        \langle \vec{v}, \vec{v}\rangle
      }{
        \langle \vec{v}, \vec{v} \rangle
      }
      =
      \lambda\,.
    \end{equation*}
  \item By again writing the Rayleigh Quotient as
    in~\eqref{eq:rq:innerprod} and using the semi-linearity in the
    first and linearity in the second argument of the inner product we
    obtain
    \begin{equation*}
      \rq_{\beta \mat{A}}(\alpha \vec{x}) = \frac{
        \langle \alpha \vec{x}, \beta \mat{A} (\alpha \vec{x} )\rangle
      }{
        \langle \alpha \vec{x}, \alpha \vec{x} \rangle
      }
      = \beta \frac{
        \overline{\alpha} \alpha \langle \vec{x}, \mat{A} \vec{x} \rangle
      }{
        \overline{\alpha} \alpha \langle\vec{x}, \vec{x} \rangle
      }
      =
      \beta \rq_{\mat{A}}(\vec{x})\,.
    \end{equation*}

  \item
    \[
      \rq_{\mat{A} - \alpha \mat{I}}(\vec{x}) = \frac{ \vec{x}^\herm (
        \mat{A} - \alpha \mat{I}) \vec{x} }{ \vec{x}^\herm \vec{x} } =
      \frac{ \vec{x}^\herm \mat{A} \vec{x} - \alpha \vec{x}^\herm
        \vec{x} }{ \vec{x}^\herm \vec{x} } = \rq_{\mat{A}}(\vec{x}) -
      \alpha \,.
    \]
  \end{enumerate}
\end{proof}

Although the Rayleigh Quotient might look arbitrary at first sight, it
occurs naturally as the solution of a least squares minimisation
problem. First note that if $(\lambda, \vec{v})$ is an eigenpair of
$\mat{A}$
\begin{equation*}
  \norm{\mat{A} \vec{v} - \lambda \vec{v}} = 0\,.
\end{equation*}
Now, suppose $\hat{\vec{v}}$ is an approximation for $\vec{v}$ and we
want to find the best approximation $\hat{\lambda}$ for $\lambda$ in
the sense that
\begin{equation*}
  \hat{\lambda} = \argmin_{\mu \in \C} \norm{\mat{A} \hat{\vec{v}} - \mu \hat{\vec{v}}}\,.
\end{equation*}
This is a linear least squares problem in $\mu$ with normal equations
(see, \eg~\cite[Theorem~6.12, p.~362]{friedberglinalg})
\begin{equation*}
  \left( \hat{\vec{v}}^\herm \hat{\vec{v}} \right) \mu = \hat{\vec{v}}^\herm \mat{A} \hat{\vec{v}}
\end{equation*}
and dividing by $\left( \hat{\vec{v}}^\herm \hat{\vec{v}} \right)$
yields the solution
\begin{equation*}
  \mu = \frac{\hat{\vec{v}}^\herm \mat{A} \hat{\vec{v}}}{ \hat{\vec{v}}^\herm \hat{\vec{v}} }
  = \rq_{\mat{A}}(\hat{\vec{v}})\,.
\end{equation*}
That is, the Rayleigh Quotient is the choice of $\mu$ that minimises
the \emph{residual} norm for the eigenvalue problem. The following
result specifies how good of an estimate the Rayleigh Quotient is.
According to Parlett this is the property to which ``the phenomenal
convergence rate [of RQI] can be attributed''~\cite[77]{Parlett1998}.
We postpone the proof of this statement until
Section~\ref{sec:convergence} where we discuss the convergence
behaviour of RQI.

\begin{lemma}[Eigenvalue estimate]%
  \label{lem:rq:quadestimate}
  Let $\vec{x} \in \C^n$ be an approximation of an eigenvector
  $\vec{v}$ of a normal\footnote{A matrix $\mat{A}$ is said to be
    \emph{normal} if $\mat{A}^\herm \mat{A} = \mat{A}
    \mat{A}^\herm$. Note that for complex Hermitian (or real
    symmetric) matrices we have $\mat{A} = \mat{A}^{\herm}$, hence
    Hermitian (and thus symmetric) matrices are normal.} matrix
  $\mat{A}$ with corresponding eigenvalue $\lambda$. Then
  \begin{equation*}
    \abs{\rq_{\mat{A}}(\vec{x}) - \rq_{\mat{A}}(\vec{v})}
    =
    \abs{\rq_{\mat{A}}(\vec{x}) - \lambda}
    =
    \mathcal{O}\left( \norm{\vec{x} - \vec{v}}^2 \right)\,.
    % \norm{\mat{A} - \lambda \mat{I}} \sin^2 \angle(\vec{x}, \vec{v})
    % \le
    % \norm{\mat{A} - \lambda \mat{I}} {\left( \frac{
    %   \norm{\vec{x} - \alpha \vec{v}}
    % }{
    %   \norm{\vec{x}}
    % }\right)}^2 \quad \text{ for all } \alpha \in \C\,.
  \end{equation*}
\end{lemma}
This result is often paraphrased as ``the Rayleigh Quotient is a
\emph{quadratically accurate} estimate of an eigenvalue'' (see, for
example,~\cite[204]{trefethen1997}). If $\mat{A}$ is non-normal, the
Rayleigh Quotient is still an estimate of order one, \ie
\begin{equation*}
  \abs{\rq_{\mat{A}}(\vec{x}) - \rq_{\mat{A}}(\vec{v})} = \mathcal{O}(\norm{\vec{x} - \vec{v}})\,.
\end{equation*}

We now have a method that allows us to obtain an estimation of an
\emph{eigenvalue} from an \emph{eigenvector}. With the Shifted Inverse
Iteration (Algorithm~\ref{alg:sii}) we have the converse: a method for
obtaining an \emph{eigenvector} estimate from an \emph{eigenvalue}
estimate. Rayleigh Quotient Iteration is essentially a combination of
those two methods where each step consists of one step of Shifted
Inverse Iteration and the computation of the Rayleigh Quotient. We
summarise the results in Algorithm~\ref{alg:rqi}.
\begin{algorithm}[htbp]
  \DontPrintSemicolon%
  \KwData{Nonzero unit vector $\vec{x}^{(0)}$}
  $\mu^{(0)} \gets {\left(\vec{x}^{(0)}\right)}^\ast \mat{A} \vec{x}^{(0)}$ \;
  \For{$k = 1,2, \dotsc$ until convergence}{
    Solve ${\left( \mat{A} - \mu^{(k)} \mat{I}\right)} \vec{y}^{(k)} = \vec{x}^{(k-1)}$ for $\vec{y}^{(k)}$\;
    $\vec{x}^{(k)} \gets \vec{y}^{(k)} / \norm{\vec{y}^{(k)}}$\;
    $\mu^{(k)} \gets {\left(\vec{x}^{(k)}\right)}^\ast \mat{A} \vec{x}^{(k)}$ \;
  } 
  \caption{Rayleigh Quotient Iteration}\label{alg:rqi}
\end{algorithm}
Note, that in the computation of the Rayleigh Quotient the division by
${(\vec{x}^{(k)})}^\ast \vec{x}^{(k)}$ can be omitted. The vector is
already normalised at the previous step so that the denominator of the
Rayleigh Quotient is equal to one.

We have yet to define what we mean by ``until convergence'' in this
Algorithm but also in the simple vector iterations from
Chapter~\ref{chapter:intro}. Now that we have defined the Rayleigh
Quotient, we can define the following stopping criterion. Run the
iteration until
\begin{equation}
  \label{eq:stopping_criterion}
  \norm{\vec{r}^{(k)}} = \norm{\mat{A}\vec{x}^{(k)} - \mu^{(k)} \vec{x}^{(k)}} < \mathtt{tol}\,,
\end{equation}
where
$\vec{r}^{(k)} \coloneqq \mat{A}\vec{x}^{(k)} - \mu^{(k)}
\vec{x}^{(k)}$ is called the \emph{residual vector} and $\mathtt{tol}$
is a user-given error tolerance. In Algorithm~\ref{alg:power:method}
(Power method) and Algorithm~\ref{alg:sii} (Inverse Iteration) the
Rayleigh Quotient of the current vector iterate
$\mu^{(k)} = {(\vec{x}^{(k)})}^\herm \mat{A} \vec{x}^{(k)}$ is not
computed. Therefore, the computation of $\mu^{(k)}$ has to be added to
these algorithms.

Obviously, if for some $k$ the tuple $(\mu^{(k)}, \vec{x}^{(k)})$ is
an eigenpair we have $\vec{r}^{(k)} = \vec{0}$. For approximate
eigenpairs we expect a small residual to imply small errors in these
approximations.  Details on this residual-based error control can be
found in~\cite[Section~3.2]{saad2011}
or~\cite[Section~5.2]{boerm}. Here, we give only some important
results without proof. A popular result, usually referred to as the
\emph{Bauer-Fike Theorem} (see, \eg~\cite[59]{saad2011}) states that
there exists an eigenvalue $\lambda$ of $\mat{A}$ such that
\begin{equation}
  \label{eq:bauer-fike}
  \abs{\lambda - \mu^{(k)}} \le \norm{\vec{r}^{(k)}}\,.
\end{equation}
Thus, if the stopping criterion is fulfilled, we have
$\abs*{\lambda - \mu^{(k)}} < \mathtt{tol}$.  For the eigenvector one
can show~\cite[63]{saad2011} that the following bound holds
\begin{equation*}
  \sin \angle(\vec{x}^{(k)}, \vec{v}) \le \frac{
    \norm{\vec{r}^{(k)}}
  }{
    \delta
  }\,,
\end{equation*}
where $\vec{v}$ is an eigenvector associated with $\lambda$ and
$\delta$ is the distance from $\mu^{(k)}$ to the rest of the spectrum,
\ie
\begin{equation*}
  \delta \coloneqq \min_i\, \{ \abs*{\mu^{(k)} - \lambda_i} \ : \ \lambda_i \neq \lambda \} \,.
\end{equation*}
Thus, if the wanted eigenvalues and its neighbours are very close,
$\delta$ is large and the eigenvector might not be accurate. Due to
the cubic convergence of RQI (which we will discuss below) instead of
stopping the iteration as soon as the criterion
in~\eqref{eq:stopping_criterion} is fulfilled, we simply run another
iteration. Often, the residual norm will then already be close to
working accuracy and the eigenvector is sufficiently accurate. If more
information about the spectrum is available, one could also adjust the
stopping criterion accordingly.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
