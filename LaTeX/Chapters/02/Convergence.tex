\section{Convergence Analysis}\label{sec:convergence}
We have mentioned on several occasions the local cubic convergence of
RQI. In this section we give a proof of this property and we also
discuss some results concerning the global convergence behaviour. We
do not give a full proof of the global convergence, since parts of it
are very technical and, at least for our purposes, it does not give
much insight in the method.

As we have seen in the previous section, the first rigorous proof of
the cubic convergence of RQI was given by Ostrowski~\cite{ostrowskiI}
but over time a number of different, simpler proofs were presented.
The one we give below follows closely the one given by
Demmel~\cite[215 \psq]{demmel}.

\begin{theorem}[Cubic convergence]%
  \label{thm:rqi:cubic}
  Rayleigh Quotient Iteration is locally cubically convergent.
\end{theorem}
Before giving the proof, we make some remarks. \emph{Locally} here
means that
\begin{enumerate}
\item it is assumed that the sequence $(\vec{x}^{(i)})_i$ does
  converge to an eigenvector of $\mat{A}$ and
\item there is a finite number of iterations for which convergence
  might not be cubic (the \emph{preasymptotic phase}). For some
  $j \in \N$, however, the $j$-th iterate is a sufficiently good
  approximation of the target eigenvector, such that the sequence of
  subsequent iterates does converge cubically.
\end{enumerate}
We defined the notion of cubic convergence in the first chapter,
nonetheless we remark again here that this means the number of correct
digits \emph{triples} at each step once the error is small enough. And
even if the method might not converge cubically from the beginning, in
practice the preasymptotic phase rarely takes more than three
steps.\todo{Citation [Parlett]}


% It suffices to make these assumptions for the eigenvectors only
% since if the vector iterates converge to an eigenvector, the
% eigenvalue approximations must also converge due to continuity of
% the Rayleigh Quotient\todo{Citation needed}.  Furthermore, we have
% already seen in Lemma~\ref{lem:rq:quadestimate} that the Rayleigh
% Quotient of good eigenvector estimates yields even better eigenvalue
% estimates.

Some of the proofs in the literature are given under the assumption
that $\mat{A}$ is diagonal. To see why no generality is lost with this
assumption, consider the eigendecomposition of $\mat{A}$ and write
$\mat{Q}^\tp \mat{A} \mat{Q} = \mat{\Lambda}$, where $\mat{Q}$ is an
orthogonal matrix consisting of the eigenvectors of $\mat{A}$ as its
columns and $\mat{\Lambda} = \diag(\lambda_1, \dotsc, \lambda_n)$ is
the diagonal matrix of eigenvalues. We discard the superscripts for a
moment and change variables in RQI\footnote{Recall the notation from
  Algorithm~\ref{alg:rqi} where $\vec{y} = \vec{y}^{(i)}$ denotes the
  unnormalised $i$-th iterate and $\vec{x} = \vec{x}^{(i)}$ denotes
  the same iterate after normalisation.}  to
$\hat{\vec{x}} \coloneqq \mat{Q}^\tp \vec{x}$ and
$\hat{\vec{y}} \coloneqq \mat{Q}^\tp \vec{y}$. Then
\begin{equation*}
  \mu^{(i)} = \mu = \rq_{\mat{A}}(\vec{x}) = \frac{
    \vec{x}^\tp \mat{A} \vec{x}
  }{
    \vec{x}^\tp \vec{x}
  } = \frac{
    \hat{\vec{x}}^\tp \mat{Q}^\tp \mat{A} \mat{Q} \hat{\vec{x}}
  }{
    \hat{\vec{x}}^\tp \mat{Q}^\tp \mat{Q} \hat{\vec{x}}
  } = \frac{
    \hat{\vec{x}}^\tp \mat{\Lambda} \hat{\vec{x}}
  }{
    \hat{\vec{x}}^\tp \hat{\vec{x}}
  } =
  \rq_{\mat{\Lambda}}(\hat{\vec{x}})
  =
  \rq_{\mat{\Lambda}}(\hat{\vec{x}}^{(i)})\,,
\end{equation*}
and
$\mat{Q} \hat{\vec{y}}^{(i+1)} = {(\mat{A} -
  \mu^{(i)}\mat{I})}^{-1}\mat{Q}\hat{\vec{x}}^{(i)}$. Hence,
\begin{equation*}
  \hat{\vec{y}}^{(i+1)} =
  \mat{Q}^\tp {(\mat{A} - \mu^{(i)}\mat{I})}^{-1}\mat{Q}\hat{\vec{x}}^{(i)} =
  {(\mat{Q}^\tp \mat{A} \mat{Q} - \mu^{(i)}\mat{I})}^{-1}\hat{\vec{x}}^{(i)} =
  {(\mat{\Lambda} - \mu^{(i)}\mat{I})}^{-1}\hat{\vec{x}}^{(i)}\,,
\end{equation*}
where we used $\mat{Q}^\tp = \mat{Q}^{-1}$, the orthogonality of
$\mat{Q}$. We see that running RQI with $\mat{A}$\todo{Explain more
  detailed} and $\vec{x}^{(0)}$ is equivalent to running RQI with
$\mat{\Lambda}$ and $\hat{\vec{x}}^{(0)}$. Thus, we assume that
$\mat{A} = \mat{\Lambda}$ is already diagonal which in particular
implies that the eigenvectors of $\mat{A}$ are $\vec{e}_i$, the
natural coordinate vectors.
\begin{proof}[Proof of Theorem~\ref{thm:rqi:cubic}]
  Suppose without loss of generality that $\vec{x}^{(i)}$ converges to
  $\vec{e}_1$. Remember that we assume that the current iterate is a
  sufficiently good estimate of $\vec{e}_1$, such that for some $i$ we
  can write $\vec{x}^{(i)} = \vec{e}_1 + \vec{d}^{(i)}$ with
  $\norm*{\vec{d}^{(i)}} = \epsilon \ll 1$. To show cubic convergence,
  we have to verify that
  \begin{equation*}
    \lim_{i \rightarrow \infty} \frac{
      \norm*{\vec{x}^{(i+1)} - \vec{e}_1}
    }{
      \norm*{\vec{x}^{(i)} - \vec{e}_1}^3
    }
    <
    M\,,
  \end{equation*}
  for some positive constant $M$. We know that
  $\norm*{\vec{x}^{(i)} - \vec{e}_1}^3 = \epsilon^3$, hence it
  suffices to show that
  $\norm*{\vec{x}^{(i+1)} - \vec{e}_1} = \mathcal{O}(\epsilon^3)$. In
  other words we have to show that
  $\vec{x}^{(i+1)} = \vec{e}_1 + \vec{d}^{(i+1)}$ with
  $\norm*{\vec{d}^{(i+1)}} = \mathcal{O}(\epsilon^3)$.

  Since the vectors are normalised at each step we have
  \begin{align*}
    1 = {(\vec{x}^{(i)})}^\tp (\vec{x}^{(i)})
    &= {(\vec{e}_1 + \vec{d}^{(i)})}^\tp (\vec{e}_1 + \vec{d}^{(i)})
      = \vec{e}_1^\tp \vec{e}_1 + 2 \vec{e}_1^\tp \vec{d}^{(i)} + {(\vec{d}^{(i)})}^\tp \vec{d}^{(i)} \\
    &= 1 + 2 d^{(i)}_1 + \epsilon^2\,.
  \end{align*}
  where $d^{(i)}_1$ denotes the first component of the vector
  $\vec{d}^{(i)}$. Rewriting gives $d^{(i)}_1 = -\epsilon^2/ 2$ and
  using the symmetry\footnote{For a symmetric matrix
    $\mat{B} = \mat{B}^\tp$ holds
    \begin{equation*}
      \vec{u}^\tp \mat{B} \vec{w} = {(\vec{u}^\tp \mat{B} \vec{w})}^\tp = \vec{w}^\tp {(\vec{u}^\tp \mat{B})}^\tp
      = \vec{w}^\tp \mat{B}^\tp \vec{u} = \vec{w}^\tp \mat{B} \vec{u} \,.
    \end{equation*}
  } of $\mat{\Lambda}$ we obtain
  \begin{align*}
    \mu^{(i)} &= {(\vec{x}^{(i)})}^\tp \mat{\Lambda}\vec{x}^{(i)}
                = {(\vec{e}_1 + \vec{d}^{(i)})}^\tp \mat{\Lambda} {(\vec{e}_1 + \vec{d}^{(i)})} \\
              &= \vec{e}_1^\tp \mat{\Lambda} \vec{e}_1 + 2\vec{e}_1^\tp \mat{\Lambda} \vec{d}^{(i)} + {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)} = \lambda_1 - \eta\,,
  \end{align*}
  where
  $\eta \coloneqq -2 \vec{e}_1^\tp \mat{\Lambda} \vec{d}^{(i)} -
  {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)} = \lambda_1
  \epsilon^2 - {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)}$.
  Using the fact that the spectral norm of a symmetric matrix is equal
  to its spectral radius, \ie the absolute value of the largest
  eigenvalue $\lambda_{\max}$, we can bound $\abs{\eta}$ as follows
  \begin{equation}
    \label{eq:eta:bound}
    \abs{\eta} \le \abs{\lambda_1} \epsilon^2 + \norm{\mat{\Lambda}}\norm*{\vec{d}^{(i)}}^2
    \le \abs{\lambda_{\max}} \epsilon^2 + \norm{\mat{\Lambda}} \epsilon^2 = 2 \norm{\mat{\Lambda}} \epsilon^2
  \end{equation}
  and we see that
  $\mu^{(i)} = \lambda_1 - \eta = \lambda_1 + \mathcal{O}(\epsilon^2)$
  (note that this proves Lemma~\ref{lem:rq:quadestimate}). We obtain
  \begin{align*}
    &\vec{y}^{(i+1)}= {(\mat{\Lambda} - \mu^{(i)})}^{-1} \vec{x}^{(i)} \\
    &= {\left(
      \frac{x^{(i)}_1}{\lambda_1 - \mu^{(i)}},
      \frac{x^{(i)}_2}{\lambda_2 - \mu^{(i)}},
      \cdots,
      \frac{x^{(i)}_n}{\lambda_n - \mu^{(i)}}
      \right)}^\tp
    & \text{(since } {(\mat{\Lambda} - \mu^{(i)}\mat{I})}^{-1} = \diag\left(\frac{1}{(\lambda_j - \mu^{(i)})}\right)\text{)} \\
    &= {\left(
      \frac{1 + d^{(i)}_1}{\lambda_1 - \mu^{(i)}},
      \frac{d^{(i)}_2}{\lambda_2 - \mu^{(i)}},
      \cdots,
      \frac{d^{(i)}_n}{\lambda_n - \mu^{(i)}},
      \right)}^\tp
    & \text{(since } \vec{x}^{(i)} = \vec{e}_1 + \vec{d}^{(i)} \text{)} \\
    &= {\left(
      \frac{1 - \epsilon^2/2 }{\eta},
      \frac{d^{(i)}_2}{\lambda_2 - \lambda_1 + \eta},
      \cdots,
      \frac{d^{(i)}_n}{\lambda_n - \lambda_1 + \eta},
      \right)}^\tp
    & \text{(since } \mu^{(i)} = \lambda_1 - \eta, \ d^{(i)}_1 = -\epsilon^2/2 \text{)} \\
    &= \frac{1 - \epsilon^2 / 2}{\eta} \,
      {\left(
      1,
      \frac{d^{(i)}_2 \eta}{(1 - \epsilon^2/2)(\lambda_2 - \lambda_1 + \eta)},
      \cdots,
      \frac{d^{(i)}_n \eta}{(1 - \epsilon^2/2)(\lambda_n - \lambda_1 + \eta)},
      \right)}^\tp \span \span \\
    &\eqqcolon \frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})\,.
  \end{align*}
  \todo{Reformat this} If we denote by $\delta$ the gap between
  $\lambda_1$ and the rest of the spectrum, \ie
  \begin{equation*}
    \delta \coloneqq \min_{j=2}^n \abs{\lambda_j - \lambda_1}\,,
  \end{equation*}
  we can bound the denominators of $\hat{\vec{d}}^{(i+1)}$ using
  $\abs{\lambda_j - \lambda_1 + \eta} \ge \delta - \abs{\eta}$, and in
  conjunction with~\eqref{eq:eta:bound} we get
  \begin{equation*}
    \norm{\hat{\vec{d}}^{(i+1)}}
    \le
    \frac{
      \norm*{\vec{d}^{(i)}} \abs*{\eta}}{
      (1 - \epsilon^2/2)(\delta - \abs*{\eta})}
    \le
    \frac{
      2\norm{\Lambda}\epsilon^3}{
      (1 - \epsilon^2/2)(\delta - 2 \norm{\Lambda}\epsilon^2)
    }
  \end{equation*}
  or $\norm*{\hat{\vec{d}}^{(i+1)}} =
  \mathcal{O}(\epsilon^3)$. Finally, since
  $\vec{x}^{(i+1)} = \vec{e}_1 + \vec{d}^{(i+1)} = \vec{y}^{(i+1)} /
  \norm*{\vec{y}^{(i+1)}}$ and
  \begin{equation*}
    \frac{
      \vec{y}^{(i+1)}}{
      \norm*{\vec{y}^{(i+1)}
      }} =
    \frac{
      \frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})
    }{
      \norm{\frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})}
    } =
    \frac{
      \vec{e}_1 + \hat{\vec{d}}^{(i+1)}
    }{
      \norm{\vec{e}_1 + \hat{\vec{d}}^{(i+1)}}
    }
  \end{equation*}
  we see that also $\vec{d}^{(i+1)} = \mathcal{O}(\epsilon^3)$, which
  concludes the proof.
\end{proof}
We see that the property that the Rayleigh Quotient yields
quadratically accurate approximations of eigenvalues is indeed crucial
in the derivation of the cubic convergence. Some other proofs, for
example the one given by Parlett~\cite[77]{Parlett1998}, proof
convergence of the \emph{error angle}
$\phi^{(k)} = \angle(\vec{x}^{(k)}, \vec{v})$, \ie he shows
\begin{equation*}
  \lim_{k \rightarrow \infty} \abs{\frac{\phi^{(k+1)}}{{\left( \phi^{(k)}\right)}^{3}}} \le 1\,.
\end{equation*}
In some texts that study inverse iteration in a more particularised
fashion, cubic convergence is proofed by combining the quadratic
accuracy of the Rayleigh Quotient together with convergence results
from inverse iteration (since a single step of RQI is equivalent to a
single step of inverse iteration with the Rayleigh quotient of the
current iterate vector chosen as the shift), see for
example~\cite[208]{trefethen1997} or~\cite[89\psq]{boerm}.

To conclude this chapter we briefly discuss some of the results
concerning the global convergence behaviour of RQI. Essential for
showing that RQI is globally convergent is the following fact which,
according to Parlett~\cite[85]{Parlett1998}, is due to
Kahan~\cite{parlettkahan}. We recall that $\vec{r}^{(k)}$ denotes the
residual at step $k$ in RQI, \ie
\begin{equation*}
  \vec{r}^{(k)} = \left(\mat{A} - \mu^{(k)}\mat{I}\right) \vec{x}^{(k)} \,.
\end{equation*}

\begin{lemma}[Monotonic residuals]\label{lem:monotoni:res}
  For every $k$ it holds
  \begin{equation*}
    \norm{\vec{r}^{(k+1)}} \le \norm{\vec{r}^{(k)}}\,.
  \end{equation*}
\end{lemma}

Before giving the proof, we note that we can relate the $k$-th and
$k+1$-th iterate of RQI by
\begin{equation*}
  \vec{x}^{(k)} = \beta \left(\mat{A} - \mu^{(k)}\mat{I}\right)\vec{x}^{(k+1)}\,,
\end{equation*}
for some $\beta$ which implies
\begin{equation*}
  \begin{aligned}
    \abs{{\left( \vec{x}^{(k)} \right)}^\herm \left(\mat{A} -
        \mu^{(k)} \mat{I}\right) \vec{x}^{(k+1)}} &= \abs{\beta^{-1}
      {\left(
          \vec{x}^{(k)} \right)}^\herm \vec{x}^{(k)}} \\
    &= \abs*{\beta^{-1}} \norm{\vec{x}^{(k)}} \\
    &= \abs*{\beta^{-1}} \norm{\beta \left(\mat{A} - \mu^{(k)} \mat{I}
      \right) \vec{x}^{(k+1)}}
  \end{aligned}
\end{equation*}
and thus
\begin{equation}
  \label{eq:xk:parallel}
  \norm{\left(\mat{A} - \mu^{(k)} \mat{I} \right) \vec{x}^{(k+1)}}
  =
  \abs{{\left( \vec{x}^{(k)} \right)}^\herm \left(\mat{A} - \mu^{(k)} \mat{I}\right) \vec{x}^{(k+1)}}\,.
\end{equation}
\begin{proof}[Proof of Lemma~\ref{lem:monotoni:res}]
  \begin{align*}
    \norm{\vec{r}^{(k+1)}} &= \norm{\left( \mat{A} - \mu^{(k+1)} \mat{I} \right) \vec{x}^{(k+1)}} \\
                           &\le \norm{\left( \mat{A} - \mu^{(k)} \mat{I} \right) \vec{x}^{(k+1)}} && \text{(Since $\mu^{(k+1)}$ minimises the residual norm)} \\
                           &= \abs{{\left( \vec{x}^{(k)} \right)}^\herm \left(\mat{A} - \mu^{(k)} \mat{I}\right) \vec{x}^{(k+1)}} && \text{(by~\eqref{eq:xk:parallel})} \\
                           &\le \norm{{\left( \vec{x}^{(k)} \right)}^\herm \left(\mat{A} - \mu^{(k)} \mat{I}\right)} \norm{\vec{x}^{(k+1)}} && \text{(by the Cauchy-Schwarz inequality)} \\
                           &= \norm{{\left( \vec{x}^{(k)} \right)}^\herm \left(\mat{A} - \mu^{(k)} \mat{I}\right)} && \text{(since $\vec{x}^{(k+1)}$ is a unit vector)} \\
                           &= \norm{\left(\mat{A} - \mu^{(k)} \mat{I}\right)\vec{x}^{(k)}} && \text{(since $\mat{A}$ is symmetric)} \\
                           &= \norm{\vec{r}^{(k)}} \,.
  \end{align*}
  
\end{proof}
\begin{theorem}[Global convergence of RQI]
  Let the RQI be applied to a normal matrix started with an arbitrary
  unit vector $\vec{x}^{(0)}$.  Then, as $k \rightarrow \infty$, the
  eigenvalue sequence $\mu^{(k)}$ converges and either
  \begin{enumerate}
  \item $(\mu^{(k)}, \vec{x}^{(k)})$ converges to an eigenpair
    $(\lambda, \vec{v})$, or
  \item the sequence $\mu^{(k)}$ converges to
    ($\lambda_p + \lambda_q)/2$ and $\vec{x}^{(k)}$ oscillates between
    vectors converging to $\vec{v}_{pq}^+$ and vectors converging to
    $\vec{v}_{pq}^-$, where
    $\vec{v}_{pq}^\pm = (\vec{v}_p \pm \vec{q}) / \sqrt{2}$, where
    $\lambda_p$ and $\lambda_q$ are eigenvalues of $\mat{A}$ with
    corresponding eigenvectors $\vec{v}_p$ and $\vec{v}_q$.
  \end{enumerate}
  The latter is unstable under perturbations of $\vec{x}^{(k)}$.
\end{theorem}

As mentioned above, we do not give a complete proof of this result;
below we outline a rough sketch of the first statement based on the
proof by Parlett in~\cite{Parlett1998}.

We begin by observing that the monotonicity of the residuals implies
\begin{gather*}
  \norm{\vec{r}^{(k)}} \longrightarrow \tau \ge 0 \quad \text{as } k
  \rightarrow \infty\,.
\end{gather*}
Now, note that the sequence of vector iterates $\vec{x}^{(k)}$ is
confined in the unit sphere, a compact subset of $\R^n$. One can also
show that the sequence of the Rayleigh Quotients
$\rq_{\mat{A}}(\vec{x}^{(k)})$ is confined in a compact set and thus,
the sequence
${\left(\rq_{\mat{A}}(\vec{x}^{(k)}), \vec{x}^{(k)}\right)}_{k \in
  \N}$ has at least one limit point. Call this point
$(\rho, \vec{z})$. This point is the limit of a subsequence
${\left(\rq_{\mat{A}}(\vec{x}^{(j)}), \vec{x}^{j}\right)}_{j \in
  \mathcal{J}}$ for some index set $\mathcal{J}$. Using the fact that
the Rayleigh Quotient is a continuous function on the unit sphere we
see that
\begin{gather*}
  \rq_{\mat{A}}(\vec{z}) = \lim_j \rq_{\mat{A}}(\vec{x}^{(j)}) = \rho
\end{gather*}
and thus
\begin{gather*}
  \norm{(\mat{A} - \rho)\vec{z}} = \lim_j \norm{\vec{r}^{(j)}} =
  \tau\,,
\end{gather*}
where the limits are for $j \rightarrow \infty$ in $\mathcal{J}$.  If
we assume $\tau = 0$, this shows that $(\rho, \vec{z})$ must be an
eigenpair of $\mat{A}$. Parlett then proceeds to use the local
convergence theorem to conclude that indeed
$\lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{z}$.

The second case $\tau > 0$ is much harder to analyse and does also
require a number of auxiliary results relating the eigenvectors of a
matrix $\mat{A}$ to its square $\mat{A}^2$. Therefore, as mentioned
above, we do not discuss this part of the proof here.

Parlett states in~\cite[61]{Parlett1998} and in~\cite[680]{parlettrqi}
that RQI converges for almost all starting vectors but Batterson and
Smillie claim that this assertion is not
proofed~\cite[625]{battersonsmillie}. They proceed to give a proof of
this fact which is formulated in their paper as follows.

\begin{theorem}
  The set of unit vectors for which RQI does not converge to an
  eigenvector is a set of measure zero.
\end{theorem}


\todo{Concluding remarks}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
