\section{Convergence Analysis}\label{sec:convergence}
We mentioned earlier the local cubic convergence of RQI. In this
section we give a proof of this property and we also discuss some
results concerning the global convergence behaviour. We do not give a
full proof of the global convergence, since parts of it are very
technical and, at least for our purposes, it does not give much
insight in the method.

As we have seen in the previous section, the first rigorous proof of
the cubic convergence of RQI was given by Ostrowski~\cite{ostrowskiI}
but over time a number of different, simpler proofs were presented.
The one we give below follows closely the one given by
Demmel~\cite[215 \psq]{demmel}.

\begin{theorem}[Cubic convergence]%
  \label{thm:rqi:cubic}
  Rayleigh Quotient Iteration is locally cubically convergent.
\end{theorem}
Before giving the proof, we make some remarks. \emph{Locally} here
means that
\begin{enumerate}
\item it is assumed that the sequence $(\vec{x}^{(i)})_i$ does
  converge to an eigenvector of $\mat{A}$ and
\item there is a finite number of iterations for which convergence
  might not be cubic (the \emph{preasymptotic phase}). For some
  $j \in \N$, however, the $j$-th iterate is a sufficiently good
  approximation of the target eigenvector, such that the sequence of
  subsequent iterates does converge cubically.
\end{enumerate}
We defined the notion of cubic convergence in the first chapter,
nonetheless we remark again here that this means the number of correct
digits \emph{triples} at each step once the error is small enough. And
even if the method might not converge cubically from the beginning, in
practice the preasymptotic phase rarely takes more than three
steps.\todo{Citation [Parlett]}


% It suffices to make these assumptions for the eigenvectors only
% since if the vector iterates converge to an eigenvector, the
% eigenvalue approximations must also converge due to continuity of
% the Rayleigh Quotient\todo{Citation needed}.  Furthermore, we have
% already seen in Lemma~\ref{lem:rq:quadestimate} that the Rayleigh
% Quotient of good eigenvector estimates yields even better eigenvalue
% estimates.

Some of the proofs in the literature are given under the assumption
that $\mat{A}$ is diagonal. To see why no generality is lost with this
assumption, consider the eigendecomposition of $\mat{A}$ and write
$\mat{Q}^\tp \mat{A} \mat{Q} = \mat{\Lambda}$, where $\mat{Q}$ is an
orthogonal matrix consisting of the eigenvectors of $\mat{A}$ as its
columns and $\mat{\Lambda} = \diag(\lambda_1, \dotsc, \lambda_n)$ is
the diagonal matrix of eigenvalues. We discard the superscripts for a
moment and change variables in RQI\footnote{Recall the notation from
  Algorithm~\ref{alg:rqi} where $\vec{y} = \vec{y}^{(i)}$ denotes the
  unnormalised $i$-th iterate and $\vec{x} = \vec{x}^{(i)}$ denotes
  the same iterate after normalisation.}  to
$\hat{\vec{x}} \coloneqq \mat{Q}^\tp \vec{x}$ and
$\hat{\vec{y}} \coloneqq \mat{Q}^\tp \vec{y}$. Then
\begin{equation*}
  \mu^{(i)} = \mu = \rq_{\mat{A}}(\vec{x}) = \frac{
    \vec{x}^\tp \mat{A} \vec{x}
  }{
    \vec{x}^\tp \vec{x}
  } = \frac{
    \hat{\vec{x}}^\tp \mat{Q}^\tp \mat{A} \mat{Q} \hat{\vec{x}}
  }{
    \hat{\vec{x}}^\tp \mat{Q}^\tp \mat{Q} \hat{\vec{x}}
  } = \frac{
    \hat{\vec{x}}^\tp \mat{\Lambda} \hat{\vec{x}}
  }{
    \hat{\vec{x}}^\tp \hat{\vec{x}}
  } =
  \rq_{\mat{\Lambda}}(\hat{\vec{x}})
  =
  \rq_{\mat{\Lambda}}(\hat{\vec{x}}^{(i)})\,,
\end{equation*}
and
$\mat{Q} \hat{\vec{y}}^{(i+1)} = {(\mat{A} -
  \mu^{(i)}\mat{I})}^{-1}\mat{Q}\hat{\vec{x}}^{(i)}$. Hence,
\begin{equation*}
  \hat{\vec{y}}^{(i+1)} =
  \mat{Q}^\tp {(\mat{A} - \mu^{(i)}\mat{I})}^{-1}\mat{Q}\hat{\vec{x}}^{(i)} =
  {(\mat{Q}^\tp \mat{A} \mat{Q} - \mu^{(i)}\mat{I})}^{-1}\hat{\vec{x}}^{(i)} =
  {(\mat{\Lambda} - \mu^{(i)}\mat{I})}^{-1}\hat{\vec{x}}^{(i)}\,,
\end{equation*}
where we used $\mat{Q}^\tp = \mat{Q}^{-1}$, the orthogonality of
$\mat{Q}$. We see that running RQI with $\mat{A}$\todo{Explain more
  detailed} and $\vec{x}^{(0)}$ is equivalent to running RQI with
$\mat{\Lambda}$ and $\hat{\vec{x}}^{(0)}$. Thus, we assume that
$\mat{A} = \mat{\Lambda}$ is already diagonal which in particular
implies that the eigenvectors of $\mat{A}$ are $\vec{e}_i$, the
natural coordinate vectors.
\begin{proof}[Proof of Theorem~\ref{thm:rqi:cubic}]
  Assume that the eigenvalue that is computed is \emph{simple}
  \todo{Define simple and highlight where this is required in the
    proof} and suppose without loss of generality that $\vec{x}^{(i)}$
  converges to $\vec{e}_1$. Remember that we assume that the current
  iterate is a sufficiently good estimate of $\vec{e}_1$, such that
  for some $i$ we can write
  $\vec{x}^{(i)} = \vec{e}_1 + \vec{d}^{(i)}$ with
  $\norm*{\vec{d}^{(i)}} = \epsilon \ll 1$. To show cubic convergence,
  we have to verify that
  \begin{equation*}
    \lim_{i \rightarrow \infty} \frac{
      \norm*{\vec{x}^{(i+1)} - \vec{e}_1}
    }{
      \norm*{\vec{x}^{(i)} - \vec{e}_1}^3
    }
    <
    M\,,
  \end{equation*}
  for some positive constant $M$. We know that
  $\norm*{\vec{x}^{(i)} - \vec{e}_1}^3 = \epsilon^3$, hence it
  suffices to show that
  $\norm*{\vec{x}^{(i+1)} - \vec{e}_1} = \mathcal{O}(\epsilon^3)$. In
  other words we have to show that
  $\vec{x}^{(i+1)} = \vec{e}_1 + \vec{d}^{(i+1)}$ with
  $\norm*{\vec{d}^{(i+1)}} = \mathcal{O}(\epsilon^3)$.

  Since the vectors are normalised at each step we have
  \begin{align*}
    1 = {(\vec{x}^{(i)})}^\tp (\vec{x}^{(i)})
    &= {(\vec{e}_1 + \vec{d}^{(i)})}^\tp (\vec{e}_1 + \vec{d}^{(i)})
      = \vec{e}_1^\tp \vec{e}_1 + 2 \vec{e}_1^\tp \vec{d}^{(i)} + {(\vec{d}^{(i)})}^\tp \vec{d}^{(i)} \\
    &= 1 + 2 d^{(i)}_1 + \epsilon^2\,.
  \end{align*}
  where $d^{(i)}_1$ denotes the first component of the vector
  $\vec{d}^{(i)}$. Rewriting gives $d^{(i)}_1 = -\epsilon^2/ 2$ and
  using the symmetry\footnote{For a symmetric matrix
    $\mat{B} = \mat{B}^\tp$ holds
    \begin{equation*}
      \vec{u}^\tp \mat{B} \vec{w} = {(\vec{u}^\tp \mat{B} \vec{w})}^\tp = \vec{w}^\tp {(\vec{u}^\tp \mat{B})}^\tp
      = \vec{w}^\tp \mat{B}^\tp \vec{u} = \vec{w}^\tp \mat{B} \vec{u} \,.
    \end{equation*}
  } of $\mat{\Lambda}$ we obtain
  \begin{align*}
    \mu^{(i)} &= {(\vec{x}^{(i)})}^\tp \mat{\Lambda}\vec{x}^{(i)}
                = {(\vec{e}_1 + \vec{d}^{(i)})}^\tp \mat{\Lambda} {(\vec{e}_1 + \vec{d}^{(i)})} \\
              &= \vec{e}_1^\tp \mat{\Lambda} \vec{e}_1 + 2\vec{e}_1^\tp \mat{\Lambda} \vec{d}^{(i)} + {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)} = \lambda_1 - \eta\,,
  \end{align*}
  where
  $\eta \coloneqq -2 \vec{e}_1^\tp \mat{\Lambda} \vec{d}^{(i)} -
  {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)} = \lambda_1
  \epsilon^2 - {(\vec{d}^{(i)})}^\tp \mat{\Lambda} \vec{d}^{(i)}$.
  Using the fact that the spectral norm of a symmetric matrix is equal
  to its spectral radius, \ie the absolute value of the largest
  eigenvalue $\lambda_{\max}$, we can bound $\abs{\eta}$ as follows
  \begin{equation}
    \label{eq:eta:bound}
    \abs{\eta} \le \abs{\lambda_1} \epsilon^2 + \norm{\mat{\Lambda}}\norm*{\vec{d}^{(i)}}^2
    \le \abs{\lambda_{\max}} \epsilon^2 + \norm{\mat{\Lambda}} \epsilon^2 = 2 \norm{\mat{\Lambda}} \epsilon^2
  \end{equation}
  and we see that
  $\mu^{(i)} = \lambda_1 - \eta = \lambda_1 + \mathcal{O}(\epsilon^2)$
  (note that this proves Lemma~\ref{lem:rq:quadestimate}). We obtain
  \begin{align*}
    &\vec{y}^{(i+1)}= {(\mat{\Lambda} - \mu^{(i)})}^{-1} \vec{x}^{(i)} \\
    &= {\left(
      \frac{x^{(i)}_1}{\lambda_1 - \mu^{(i)}},
      \frac{x^{(i)}_2}{\lambda_2 - \mu^{(i)}},
      \cdots,
      \frac{x^{(i)}_n}{\lambda_n - \mu^{(i)}}
      \right)}^\tp
    & \text{(since } {(\mat{\Lambda} - \mu^{(i)}\mat{I})}^{-1} = \diag\left(\frac{1}{(\lambda_j - \mu^{(i)})}\right)\text{)} \\
    &= {\left(
      \frac{1 + d^{(i)}_1}{\lambda_1 - \mu^{(i)}},
      \frac{d^{(i)}_2}{\lambda_2 - \mu^{(i)}},
      \cdots,
      \frac{d^{(i)}_n}{\lambda_n - \mu^{(i)}},
      \right)}^\tp
    & \text{(since } \vec{x}^{(i)} = \vec{e}_1 + \vec{d}^{(i)} \text{)} \\
    &= {\left(
      \frac{1 - \epsilon^2/2 }{\eta},
      \frac{d^{(i)}_2}{\lambda_2 - \lambda_1 + \eta},
      \cdots,
      \frac{d^{(i)}_n}{\lambda_n - \lambda_1 + \eta},
      \right)}^\tp
    & \text{(since } \mu^{(i)} = \lambda_1 - \eta, \ d^{(i)}_1 = -\epsilon^2/2 \text{)} \\
    &= \frac{1 - \epsilon^2 / 2}{\eta} \,
      {\left(
      1,
      \frac{d^{(i)}_2 \eta}{(1 - \epsilon^2/2)(\lambda_2 - \lambda_1 + \eta)},
      \cdots,
      \frac{d^{(i)}_n \eta}{(1 - \epsilon^2/2)(\lambda_n - \lambda_1 + \eta)},
      \right)}^\tp \span \span \\
    &\eqqcolon \frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})\,.
  \end{align*}
  If we denote by $\delta$ the gap between $\lambda_1$ and the rest of
  the spectrum, \ie
  \begin{equation*}
    \delta \coloneqq \min_{j=2}^n \abs{\lambda_j - \lambda_1}\,,
  \end{equation*}
  we can bound the denominators of $\hat{\vec{d}}^{(i+1)}$ using
  $\abs{\lambda_j - \lambda_1 + \eta} \ge \delta - \abs{\eta}$, and in
  conjunction with~\eqref{eq:eta:bound} we get
  \begin{equation*}
    \norm{\hat{\vec{d}}^{(i+1)}}
    \le
    \frac{
      \norm*{\vec{d}^{(i)}} \abs*{\eta}}{
      (1 - \epsilon^2/2)(\delta - \abs*{\eta})}
    \le
    \frac{
      2\norm{\Lambda}\epsilon^3}{
      (1 - \epsilon^2/2)(\delta - 2 \norm{\Lambda}\epsilon^2)
    }
  \end{equation*}
  or $\norm*{\hat{\vec{d}}^{(i+1)}} =
  \mathcal{O}(\epsilon^3)$. Finally, since
  $\vec{x}^{(i+1)} = \vec{e}_1 + \vec{d}^{(i+1)} = \vec{y}^{(i+1)} /
  \norm*{\vec{y}^{(i+1)}}$ and
  \begin{equation*}
    \frac{
      \vec{y}^{(i+1)}}{
      \norm*{\vec{y}^{(i+1)}
      }} =
    \frac{
      \frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})
    }{
      \norm{\frac{1 - \epsilon^2 / 2}{\eta}\, (\vec{e}_1 + \hat{\vec{d}}^{(i+1)})}
    } =
    \frac{
      \vec{e}_1 + \hat{\vec{d}}^{(i+1)}
    }{
      \norm{\vec{e}_1 + \hat{\vec{d}}^{(i+1)}}
    }
  \end{equation*}
  we see that also $\vec{d}^{(i+1)} = \mathcal{O}(\epsilon^3)$, which
  concludes the proof.
\end{proof}
We see that the property that the Rayleigh Quotient yields
quadratically accurate approximations of eigenvalues is indeed crucial
in the derivation of the cubic convergence. Some other proofs, for
example the one given by Parlett~\cite[77]{Parlett1998}, proof
convergence of the \emph{error angle}
$\phi^{(k)} = \angle(\vec{x}^{(k)}, \vec{v})$, \ie
\begin{equation*}
  \lim_{k \rightarrow \infty} \abs{\frac{\phi^{(k+1)}}{{\left( \phi^{(k)}\right)}^{3}}} \le 1\,.
\end{equation*}
In some texts that study inverse iteration in a more particularised
fashion, cubic convergence is proofed by combining the quadratic
accuracy of the Rayleigh Quotient together with convergence results
from inverse iteration (since a single step of RQI is equivalent to a
single step of inverse iteration with the Rayleigh quotient of the
current iterate vector chosen as the shift), see for
example~\cite[208]{trefethen1997} or~\cite[89\psq]{boerm}.

\todo{Global convergence behaviour}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../main"
%%% End:
