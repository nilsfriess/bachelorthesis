\chapter{Complex Rayleigh Quotient Iteration}
In this chapter we introduce \emph{Complex Rayleigh Quotient
  Iteration} (CRQI\footnote{We abbreviate the classic Rayleigh
  Quotient Iteration that was discussed in the previous chapter by
  \emph{RQI} or \emph{classic RQI} and the method introduced in this
  chapter by \emph{CRQI}.}). This is a novel shift-and-invert
algorithm similar to classic RQI. Numerical examples suggest that that
this new method overcomes some of the disadvantages of classic RQI.
\todo{Finish intro for chapter}

\section{Motivation}
As was the case in the previous chapters we fix a real symmetric
matrix $\mat{A} \in \R^{n \times n}$. Recall that since the
eigenvectors $\vec{v}_1, \dotsc, \vec{v}_n$ of $\mat{A}$ form an
orthonormal basis of $\R^n$ we can write every $\vec{u} \in \R^n$ as
\[
  \vec{u} = \sum_{i=1}^n \alpha_i \vec{v}_i
\]
for certain $\alpha_1, \dotsc, \alpha_n \in \R$. Suppose now, that
$\vec{u}$ is a good approximation for one of the eigenvectors, say for
$\vec{v}_k$. Then
\[
  \alpha_k \approx 1 \qquad \text{and} \qquad \alpha_j \approx 0 \
  \text{ for } \ j \neq k \,.
\]
Due to the pairwise orthogonality of the eigenvectors this implies
\begin{equation}
  \label{eq:guess_orthogonal}
  \vec{u}^\tp \vec{v}_j \approx
  \begin{cases}
    1 & \text{ if } j = k \,, \\
    0 & \text{ if } j \neq k \,.
  \end{cases}
\end{equation}
As we have seen before, when using classic RQI, even good
approximations of eigenvectors can lead to convergence to the wrong
eigenpair when the gap between the target eigenvalue and eigenvalues
nearby is very small. The main idea of CRQI is now to use the
approximation $\vec{u}$ of $\vec{v}_k$ to perturb the linear system
that is solved at each step in RQI in such a way that the distance
between the target eigenvalue $\lambda_k$ and the neighbouring
eigenvalues is increased. Of course this perturbed linear system will
lead to wrong solutions and so we will ``decrease'' this perturbation
successively until we arrive at the unperturbed problem.
\todo{Rewrite} We make use of the fact that all eigenvalues of
$\mat{A}$ are real and perturb this matrix in such a way that the
eigenvalues are ``raised'' into the complex plane. Of course, we do
not want to raise them all equally but rather in such a way that the
Euclidean distance between the target eigenvalue and the other
eigenvalue is increased.

The idea is now to apply Rayleigh Quotient iteration to the matrix
\begin{equation}
  \label{eq:a:tilde}
  \tilde{\mat{A}} \coloneqq \mat{A} - \gamma i(\mat{I} - \vec{u} \vec{u}^\tp)
\end{equation}
instead of $\mat{A}$, where $\gamma > 0$ is positive real number and
$i$ denotes the imaginary unit. To get an intuition on why we choose
this matrix, we first note that the matrix
$\mat{I} - \vec{u} \vec{u}^\tp$ defines the orthogonal projection onto
the span of $\vec{u}$.

% Of course, this works for any vector $\vec{v} \in \R^n$ and can be
% seen as follows.  Given a vector $\vec{x} \in \R^n$, the projection
% of $\vec{x} \in \R^n$ onto the span of $\vec{v}$ is
% \begin{equation*}
%   \frac{
%     \langle \vec{v}, \vec{x} \rangle
%   }{
%     \langle \vec{v}, \vec{v} \rangle
%   }\vec{v} \,,
% \end{equation*}
% where $\langle \cdot, \cdot \rangle$ denotes the Euclidean inner
% product on $\R^n$. Thus, the projection onto the orthogonal
% complement of the span of $\vec{v}$ is
% \begin{equation*}
%   \vec{x} - (\vec{v}^\tp \vec{x}) \vec{v} = \vec{x} - (\vec{v} \vec{v}^\tp) \vec{x} = (\mat{I} - \vec{v} \vec{v}^\tp) \vec{x} \,.
% \end{equation*}
% \todo{Look up projection formulas, search references and correct
% derviation}

Therefore, a vector $\vec{x}$ that is almost parallel to $\vec{u}$
will barely ``see'' the imaginary part
$\gamma i (\mat{I} - \vec{u} \vec{u}^\tp)$ when multiplied with
$\tilde{\mat{A}}$ and so
$\tilde{\mat{A}} \vec{x} \approx \mat{A}\vec{x}$. If, however, the
vector $\vec{x}$ is almost perpendicular to $\vec{u}$ we have
\begin{equation}
  \label{eq:a:tilde:multorth}
  \tilde{\mat{A}} \vec{x} = \mat{A}\vec{x} - \gamma i \vec{x} - \vec{u}
  \underbrace{\vec{u}^\tp \vec{x}}_{\approx 0} \approx (\mat{A} - \gamma i \mat{I})\vec{x}\,.
\end{equation}
Since $u$ approximates $\vec{v}_k$, the orthogonal complement of the
span of $\vec{u}$ approximates the orthogonal complement of the span
by $\vec{v}_k$. The latter, due to the orthogonality of the
eigenvectors, is the subspace spanned by the remaining eigenvectors.
Therefore, we expect that the eigenvectors of $\tilde{\mat{A}}$ are
similar to those of $\mat{A}$ and that the eigenvalues corresponding
to eigenvectors $\vec{v}_j$, $j \neq k$ to approximately be
$\lambda_j - \gamma i$ due to~\eqref{eq:a:tilde:multorth}. The
eigenvalue corresponding to $\vec{v}_k$ would then be approximately
equal to $\lambda_k$ since
$\tilde{\mat{A}}\vec{v}_k \approx \mat{A}\vec{v}_k = \lambda_k
\vec{v}_k$.

To make this intuition more quantitative, we first need the following
result where we replace $\vec{u}$ by the exact target eigenvector
$\vec{v}_k$.
\begin{lemma}
  \label{lem:eigs:atilde0}
  The matrix
  $\tilde{\mat{A}}^{(0)} \coloneqq \mat{A} - \gamma i(\mat{I}-
  \vec{v}_k \vec{v}_k^\tp)$ has the same eigenvectors as $\mat{A}$
  with corresponding eigenvalues
  $\lambda_j(\tilde{\mat{A}}^{(0)}) = \lambda_j(\mat{A}) - \gamma i$
  for $j \neq k$ and
  $\lambda_k(\tilde{\mat{A}}^{(0)}) = \lambda_k(\mat{A})$.
\end{lemma}

\begin{proof}
  First consider $j \neq k$. A straightforward calculation yields
  \begin{equation*}
    (\mat{A} - \gamma i(\mat{I}- \vec{v}_k \vec{v}_k^\tp)) \vec{v}_j = \mat{A}\vec{v}_j - \gamma i \vec{v}_j + \gamma i \vec{v}_k \underbrace{\vec{v}_k^\tp  \vec{v}_j}_{ = 0}
    = \lambda_j \vec{v}_j - \gamma i \vec{v}_j = (\lambda_j - \gamma i) \vec{v}_j \,.   
  \end{equation*}
  For $j = k$ we have
  $\vec{v}_k^\tp \vec{v}_j = \vec{v}_k^\tp \vec{v}_k = 1$ and thus
  \begin{equation*}
    (\mat{A} - \gamma i(\mat{I}- \vec{v}_k \vec{v}_k^\tp)) \vec{v}_k = \lambda_k \vec{v}_k - \gamma i \vec{v}_k + \gamma i \vec{v}_k = \lambda_k \vec{v}_k\,.
  \end{equation*}
\end{proof}
We now decompose $\tilde{\mat{A}}$ into the sum
$\tilde{\mat{A}} = \tilde{\mat{A}}^{(0)} + \tilde{\mat{A}}^{(1)}$,
where $\tilde{\mat{A}}^{(0)}$ is defined in
Lemma~\ref{lem:eigs:atilde0} and $\tilde{\mat{A}}^{(1)}$ is given by
\begin{equation*}
  \tilde{\mat{A}}^{(1)} \coloneqq \gamma i ( \vec{u} \vec{u}^\tp - \vec{v}_k\vec{v}_k^\tp)\,.
\end{equation*}
% Weyl`s nequality then gives the bound
% \begin{equation}
%   \label{eq:weyl}
%   \max_{k = 1}^n\, \abs*{\lambda_k(\mat{\tilde{A}}) - \lambda_k(\tilde{\mat{A^{0}}})} \le \norm{\tilde{\mat{A}}^{(1)}}\,.
% \end{equation}
% To compute the norm on the right hand side of~\eqref{eq:weyl} we
% make use of the following well-known facts from linear algebra.
% \begin{proposition}
%   Let $\mat{A} \in \R^{n \times n}$ be a real symmetric matrix. Then
%   \begin{enumerate}[label=(\alph*)]
%   \item The sum of the eigenvalues of $\mat{A}$ is equal to the
%     \emph{trace} of $\mat{A}$ that is defined as the sum of its
%     diagonal entries, \ie,
%     \begin{equation*}
%       \sum_{j = 1}^n \lambda_j(\mat{A}) = \sum_{k = 1}^n \mat{A}_{kk}\,.
%     \end{equation*}
%   \end{enumerate}
% \end{proposition}


% used as an initial guess in the classic Rayleigh Quotient Iteration.
% We can, however, use~\eqref{eq:guess_orthogonal} to shift the
% eigenvalues in such a way that the distance between the target
% eigenvalue and adjacent eigenvalues is increased. This fact is then
% used to alter the linear system that is solved at each step in the
% Rayleigh Quotient Iteration, increasing the chance that the method
% will converge to the right eigenpair.

% We first observe that given a vector $\vec{x} \in \R^n$, the
% projection of $\vec{x} \in \R^n$ onto the span of a unit vector
% $\vec{u}$ is
% \[
%   (\vec{u}^\tp \vec{x})\vec{u} = \vec{u}(\vec{u}^\tp
%   \vec{x})=(\vec{u} \vec{u}^\tp) \vec{x}\,,
% \]
% Thus, the projection onto the orthogonal complement of the span of
% $\vec{u}$ is
% \[
%   \vec{x} - (\vec{u}^\tp \vec{x}) \vec{u} = \vec{x} - (\vec{u}
%   \vec{u}^\tp) \vec{x} = (\mat{I} - \vec{u} \vec{u}^\tp) \vec{x} \,.
% \]
% Consider the matrix
% $\tilde{\mat{A}} \coloneqq \mat{A} - \gamma i ( \mat{I} - \vec{u}
% \vec{u}^\tp)$ where $\gamma > 0$ is an arbitrary real number and $i$
% denotes the imaginary unit. Multiplying this matrix with an
% eigenvector of $\mat{A}$ gives
% \begin{align*}
%   \tilde{\mat{A}}\vec{v}_j &= \left(  \mat{A} - \gamma i ( \mat{I} - \vec{v}_k
%                              \vec{v}_k^\tp) \right) \vec{v}_j \\
%                            &= \mat{A} \vec{v}_j - \gamma i \vec{v}_j + \gamma i \vec{v}_k \vec{v}_k^\tp \vec{v}_j \\
%                            &= (\lambda_j - \gamma i)\vec{v}_j + \gamma i \delta_{kj} \vec{v}_k \,,
% \end{align*}
% where $\delta_{kj}$ denotes the Kronecker delta. In other words, if
% $j \neq k$ we have
% \[
%   \tilde{\mat{A}} \vec{v}_j = (\lambda_j - \gamma i)\vec{v}_j
% \]
% and if $j = k$ we have
% \[
%   \tilde{\mat{A}} \vec{v}_j = \tilde{\mat{A}} \vec{v}_k = (\lambda_k
%   - \gamma i) \vec{v}_k - \gamma i \vec{u} = \lambda_k \vec{v}_k \,.
% \]
% We can conclude that $\tilde{\mat{A}}$ has the same set of
% eigenvectors as $\mat{A}$ with corresponding eigenvalues
% $\lambda_j - \gamma i$ if $j \neq k$ and $\lambda_j$ if $j = k$. In
% other words, we ``raised'' all eigenvalues corresponding to
% eigenvectors that are orthogonal to $\vec{u}$ from the real line
% into the complex plane such that their imaginary parts are equal to
% $\gamma$ increasing the distance \wrt the Euclidean norm between the
% eigenvalues.

% Note, however, that $\vec{u}$ is an eigenvector of $\mat{A}$. We
% will shortly use this approach to guarantee convergence to the
% target eigenvector using a Rayleigh Quotient type Iteration. Of
% course, we will merely have an approximation of an eigenvector so
% that~\eqref{eq:guess_orthogonal} holds. However, as the observations
% above suggest, this approach also works for such
% approximations. This is due to the fact that vectors that lie in the
% orthogonal complement of the span of an approximate eigenvector are
% themselves approximate eigenvectors. The eigenvalues are then raised
% according to the angle between the corresponding eigenvector and the
% subspace defined by the orthogonal projection of the span of
% $\vec{u}$ (see the Lemma below).  Thus, the target eigenvalue (\ie
% the eigenvalue corresponding to the eigenvector closest to
% $\vec{u}$) is barely raised.


% \begin{lemma}\label{lem:eig:orthogprojection}
%   Define the matrix
%   $\tilde{\mat{A}} \coloneqq \mat{A} - \gamma i (\mat{I} -
%   \vec{u}\vec{u}^\tp)$.
% \end{lemma}


% \begin{lemma}
%   Let $\mat{A} \in \R^{n \times n}$ be a nonsingular real symmetric
%   matrix. Let $\vec{u} \in \R^n$ be a unit vector. We define the
%   matrices
%   \begin{equation}
%     \label{eq:rayleigh_quotient_complex}
%     \mat{B} \coloneqq {\left(\mat{A} - (\sigma - \gamma i) \mat{I} \right)}
%   \end{equation}
%   and
%   \begin{equation}
%     \label{eq:rayleigh_quotient_proj}
%     \mat{C} \coloneqq {\left( \mat{A} - \sigma \mat{I} + \gamma i( \mat{I} - \vec{u} \vec{u}^\tp) \right)}\,,
%   \end{equation}
%   where $i$ is the imaginary unit and $\sigma, \gamma > 0$ are
%   positive real numbers such that $\sigma$ is not an eigenvalue of
%   $\mat{A}$. Then
%   \begin{equation}
%     \label{eq:rq_equal}
%     \rq_{\mat{A}}(\mat{B}^{-1} \vec{u}) = \rq_{\mat{A}}(\mat{C}^{-1} \vec{u} ) \,.
%   \end{equation}
% \end{lemma}
% \begin{proof}
%   Without loss of generality, we can assume $\sigma = 0$. Otherwise,
%   set $\tilde{\mat{A}} = \mat{A} - \sigma \mat{I}$ and use this
%   matrix instead of $\mat{A}$ ($\tilde{\mat{A}}$ is obviously still
%   real and symmetric and invertible since $\sigma$ is not an
%   eigenvalue of $\mat{A}$). First, observe that
%   $\mat{C} = \mat{B} - \gamma i \vec{u}\vec{u}^\tp$. Using the
%   Sherman-Morrison formula \parencite[50]{golub1996} and letting
%   $\alpha \coloneqq 1 - \gamma i \vec{u}^\tp \mat{B}^{-1} \vec{u} \,
%   \in \C$ we obtain
%   \begin{align*}
%     \mat{C}^{-1} \vec{u} &= {\left( \mat{B} - \gamma i \vec{u} \vec{u}^\tp \right)}^{-1} \vec{u} \\
%                          &= \left(
%                            \mat{B}^{-1} + \alpha^{-1} \mat{B}^{-1} \vec{u} \gamma i \vec{u}^\tp \mat{B}^{-1}
%                            \right) \vec{u} \\
%                          &= \mat{B}^{-1} \vec{u} + \mat{B}^{-1} \vec{u} \underbrace{\alpha^{-1} \gamma i \vec{u}^\tp \mat{B}^{-1} \vec{u} }_{ \in \C} \\
%                          &= \mat{B}^{-1} \vec{u} ( 1 + \alpha^{-1} \gamma i \vec{u}^\tp \mat{B}^{-1} \vec{u}) \,.    
%   \end{align*}
%   Thus, the vector $\mat{C}^{-1} \vec{u}$ is a scalar multple of
%   $\mat{B}^{-1} \vec{u}$ and the result follows from
%   Lemma~\ref{lem:rq:properties} (i) (the Homogenity of the Rayleigh
%   Quotient).
% \end{proof}

% \begin{corollary}
%   The Rayleigh sequences generated using the matrices $\mat{B}$ and
%   $\mat{C}$ as in~\eqref{eq:rayleigh_quotient_complex}
%   and~\eqref{eq:rayleigh_quotient_proj}, respectively, are the same.
% \end{corollary}

% \section{Preconditioners for the Complex Symmetric System}

% \section{Numerical Examples}
% \subsection{Preconditioned Large Sparse System}


%%% Local Variables:
%%% mode: latex 
%%% TeX-master: "../../main"
%%% End:
 