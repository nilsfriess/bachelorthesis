\chapter{Complex Rayleigh Quotient Iteration}
In this chapter we introduce \emph{Complex Rayleigh Quotient
  Iteration} (CRQI\footnote{We abbreviate the classic Rayleigh
  Quotient Iteration that was discussed in the previous chapter by
  \emph{RQI} or \emph{classic RQI} and the method introduced in this
  chapter by \emph{CRQI}.}). This is a novel shift-and-invert
algorithm similar to classic RQI. Numerical examples suggest that that
this new method overcomes some of the disadvantages of classic RQI.
\todo{Finish intro for chapter}

\section{Motivation}
As was the case in the previous chapters we fix a real symmetric
matrix $\mat{A} \in \R^{n \times n}$. Recall that since the
eigenvectors $\vec{v}_1, \dotsc, \vec{v}_n$ of $\mat{A}$ form an
orthonormal basis of $\R^n$ we can write every $\vec{u} \in \R^n$ as
\[
  \vec{u} = \sum_{i=1}^n \alpha_i \vec{v}_i
\]
for certain $\alpha_1, \dotsc, \alpha_n \in \R$. Suppose now, that
$\vec{u}$ is a good approximation for one of the eigenvectors, say for
$\vec{v}_k$. Then
\[
  \alpha_k \approx 1 \qquad \text{and} \qquad \alpha_j \approx 0 \
  \text{ for } \ j \neq k \,.
\]
Due to the pairwise orthogonality of the eigenvectors this implies
\begin{equation}
  \label{eq:guess_orthogonal}
  \vec{u}^\tp \vec{v}_j \approx
  \begin{cases} 
    1 & \text{ if } j = k \,, \\
    0 & \text{ if } j \neq k \,.
  \end{cases}
\end{equation}
As we have seen before, when using classic RQI, even good
approximations of eigenvectors can lead to convergence to the wrong
eigenpair when the gap between the target eigenvalue and eigenvalues
nearby is very small. The main idea of CRQI is now to use the
approximation $\vec{u}$ of $\vec{v}_k$ to perturb the linear system
that is solved at each step in RQI in such a way that the distance
between the target eigenvalue $\lambda_k$ and the neighbouring
eigenvalues is increased. In other words, we try to isolate the target
eigenvalue. Of course this perturbed linear system will lead to wrong
solutions and so we will ``decrease'' this perturbation successively
until we arrive at the unperturbed problem.

We make use of the fact that all eigenvalues of $\mat{A}$ are real and
perturb $\mat{A}$ in such a way that the eigenvalues are ``raised''
into the complex plane. Of course, we do not want to raise them all
equally but rather in such a way that the Euclidean distance between
the target eigenvalue and the other eigenvalue is increased. This is
done by incorporating the approximation $\vec{u}$ of the target
eigenvector $\vec{v}_k$ into the working matrix. Let
\begin{equation}
  \label{eq:a:tilde}
  \tilde{\mat{A}} \coloneqq \mat{A} - \gamma i(\mat{I} - \vec{u} \vec{u}^\tp)\,,
\end{equation}
where $\gamma > 0$ is positive real number and $i$ denotes the
imaginary unit. Note that the matrix $\mat{I} - \vec{u} \vec{u}^\tp$
defines the orthogonal projection onto the span of $\vec{u}$.
Therefore, a vector $\vec{x}$ that is almost parallel to $\vec{u}$
will barely ``see'' the imaginary part
$\gamma i (\mat{I} - \vec{u} \vec{u}^\tp)$ when multiplied with
$\tilde{\mat{A}}$ and so
$\tilde{\mat{A}} \vec{x} \approx \mat{A}\vec{x}$. If, however, the
vector $\vec{x}$ is almost perpendicular to $\vec{u}$ we have
\begin{equation}
  \label{eq:a:tilde:multorth}
  \tilde{\mat{A}} \vec{x} = \mat{A}\vec{x} - \gamma i \vec{x} - \vec{u}
  \underbrace{\vec{u}^\tp \vec{x}}_{\approx 0} \approx (\mat{A} - \gamma i \mat{I})\vec{x}\,.
\end{equation}
Since $u$ approximates $\vec{v}_k$, the orthogonal complement of the
span of $\vec{u}$ approximates the orthogonal complement of the span
by $\vec{v}_k$. The latter, due to the orthogonality of the
eigenvectors, is the subspace spanned by the remaining eigenvectors.
Therefore, we expect that the eigenvectors of $\tilde{\mat{A}}$ are
similar to those of $\mat{A}$ and that the eigenvalues corresponding
to eigenvectors $\vec{v}_j$, $j \neq k$ to approximately be
$\lambda_j - \gamma i$ due to~\eqref{eq:a:tilde:multorth}. The
eigenvalue corresponding to $\vec{v}_k$ would then be approximately
equal to $\lambda_k$ since
$\tilde{\mat{A}}\vec{v}_k \approx \mat{A}\vec{v}_k = \lambda_k
\vec{v}_k$.

If we would use this matrix in RQI, the results would of course not be
the target eigenpair but an eigenpair of $\tilde{\mat{A}}$. Thus,
instead of keeping this matrix fixed, we replace the vector $\vec{u}$
by the current iterate $\vec{x}^{(k)}$ and the scalar $\gamma$ by a
sequence $\gamma^{(k)}$ that converges to zero. Ideally, in the
beginning of the iteration $\gamma^{(k)}$ should be sufficiently large
such that the target eigenvalue is properly isolated. As the the
iterates get closer to the target eigenpair, $\gamma^{(k)}$ should
decrease such that in the end $\tilde{\mat{A}} \approx \mat{A}$. An
obvious choice is the norm of the current residual $\vec{r}^{(k)}$ or
related quantities such as the square of the residual norm. How the
choice of this shift influences the convergence behaviour is discussed
in the next first example of Section~\ref{sec:crqi:experiments}.

\section{Implementation}

Altough we will soon slightly change the matrix $\tilde{\mat{A}}$
defined in~\eqref{eq:a:tilde} again, we summarise the discussion up to
this point in Algorithm~\ref{alg:crqi:proj}.\footnote{By
  $\Re(\vec{x})$ we denote the real part of the complex vector
  $\vec{x}$.}

\begin{algorithm}[htpb]
  \DontPrintSemicolon%
  \KwData{Nonzero initial vector $\vec{x}^{(0)}$ with $\norm*{\vec{x}^{(0)}} = 1$}
  \Begin{
    $\mu^{(0)} \gets {(\vec{x}^{(0)})}^\herm \mat{A} \vec{x}^{(0)}$\;
    $r^{(0)} \gets \norm*{(\mat{A} - \mu^{(0)}\mat{I}) \vec{x}^{(0)}}$\;
    $\gamma^{(0)} \gets r^{(0)}$\;
    \For{$k = 1,2,\dotsc$ until convergence}{
      $\tilde{\mat{A}} \gets \mat{A} - \gamma^{(k)}i(\mat{I} - \vec{x}^{(k)} {(\vec{x}^{(k)})}^\herm)$\;
      Solve $( \tilde{\mat{A}} - \mu^{(k)}\mat{I})\tilde{\vec{x}}^{(k+1)} = \vec{x}^{(k)}$\;
      $\vec{x}^{(k+1)} \gets \tilde{\vec{x}}^{(k+1)} / \norm*{\tilde{\vec{x}}^{(k+1)}}$\;
      $\mu^{(k+1)} \gets {(\vec{x}^{(k+1)})}^\herm \mat{A} \vec{x}^{(k+1)}$\;
      $r^{(k+1)} \gets \norm*{(\mat{A} - \mu^{(k+1)}\mat{I}) \vec{x}^{(k+1)}}$\;
      $\gamma^{(k+1)} \gets r^{(k+1)}$\;
    }
    $\vec{x} \gets \Re({\vec{x}^{(k+1)}})$\;
    $\vec{x} \gets \vec{x} / \norm*{\vec{x}}$
  }
  \caption{Complex Rayleigh Quotient Iteration, First Version}\label{alg:crqi:proj}
\end{algorithm}
\todo{Sign of shift in Algo step should be +?}

Strictly speaking, this algorithm is not a Rayleigh Quotient Iteration
as the matrix changes at each step. Obviously, the method is strongly
linked to RQI so we still refer to it as Complex Rayleigh Quotient
Iteration. As mentioned above, this is not the final version as we
wish to present it. The following lemma allows for a small
simplification of the algorithm.

\begin{lemma}
  Let $\mat{A} \in \R^{n \times n}$ be a nonsingular real symmetric
  matrix. Let $\vec{u} \in \C^n$ be a unit vector. We define the
  matrices
  \begin{equation}
    \label{eq:rayleigh_quotient_complex}
    \mat{B} \coloneqq \mat{A} - (\sigma - \gamma i) \mat{I}
  \end{equation}
  and
  \begin{equation}
    \label{eq:rayleigh_quotient_proj}
    \mat{C} \coloneqq { \mat{A} - \sigma \mat{I} + \gamma i( \mat{I} - \vec{u} \vec{u}^\herm)}\,,
  \end{equation}
  where $i$ is the imaginary unit and $\sigma, \gamma > 0$ are
  positive real numbers such that $\sigma$ is not an eigenvalue of
  $\mat{A}$. Then
  \begin{equation}
    \label{eq:rq_equal}
    \rq_{\mat{A}}(\mat{B}^{-1} \vec{u}) = \rq_{\mat{A}}(\mat{C}^{-1} \vec{u} ) \,.
  \end{equation}
\end{lemma}
\begin{proof}
  Without loss of generality, we can assume $\sigma = 0$. Otherwise,
  set $\tilde{\mat{A}} = \mat{A} - \sigma \mat{I}$ and use this matrix
  instead of $\mat{A}$ ($\tilde{\mat{A}}$ is obviously still real and
  symmetric and invertible since $\sigma$ is not an eigenvalue of
  $\mat{A}$). First, observe that
  $\mat{C} = \mat{B} - \gamma i \vec{u}\vec{u}^\herm$. Using the
  Sherman-Morrison formula~\cite{hager1989update} and letting
  $\alpha \coloneqq 1 - \gamma i \vec{u}^\herm \mat{B}^{-1} \vec{u} \,
  \in \C$ we obtain
  \begin{align*}
    \mat{C}^{-1} \vec{u} &= {\left( \mat{B} - \gamma i \vec{u} \vec{u}^\herm \right)}^{-1} \vec{u} \\
                         &= \left(
                           \mat{B}^{-1} + \alpha^{-1} \mat{B}^{-1} \vec{u} \gamma i \vec{u}^\herm \mat{B}^{-1}
                           \right) \vec{u} \\
                         &= \mat{B}^{-1} \vec{u} + \mat{B}^{-1} \vec{u} \underbrace{\alpha^{-1} \gamma i \vec{u}^\herm \mat{B}^{-1} \vec{u} }_{ \in \C} \\
                         &= \mat{B}^{-1} \vec{u} ( 1 + \alpha^{-1} \gamma i \vec{u}^\herm \mat{B}^{-1} \vec{u}) \,.    
  \end{align*}
  Thus, the vector $\mat{C}^{-1} \vec{u}$ is a scalar multple of
  $\mat{B}^{-1} \vec{u}$ and the result follows from
  Lemma~\ref{lem:rq:properties} (i) (the Homogenity of the Rayleigh
  Quotient).
\end{proof}
\todo{Where is the non-singularity needed?}

Immediately, the following result follows.

\begin{corollary}
  Algorithm~\ref{alg:crqi:proj} produces the same results when
  $\tilde{\mat{A}}$ is replaced with
  $\hat{\mat{A}} = \mat{A} + \gamma i \mat{I}$.
\end{corollary}
\begin{proof}
  
\end{proof}
This result allows for a simplification of the algorithm. We combine
the real shift $\mu^{(k)}$ and the imaginary shift $i \gamma^{(k)}$
into a new shift $\sigma^{(k)} \coloneqq \mu^{(k)} - i \gamma^{(k)}$
and solve the system
\begin{equation*}
  (\mat{A} - \sigma^{(k)} \mat{I}) \vec{x}^{(k+1)} = \vec{x}^{(k)}
\end{equation*}
for $\vec{x}^{(k+1)}$ at each step.



\input{Chapters/03/NumericalExperiments}

% To make this intuition more quantitative, we first need the
% following result where we replace $\vec{u}$ by the exact target
% eigenvector $\vec{v}_k$.
% \begin{lemma}
%   \label{lem:eigs:atilde0}
%   The matrix
%   $\tilde{\mat{A}}^{(0)} \coloneqq \mat{A} - \gamma i(\mat{I}-
%   \vec{v}_k \vec{v}_k^\tp)$ has the same eigenvectors as $\mat{A}$
%   with corresponding eigenvalues
%   $\lambda_j(\tilde{\mat{A}}^{(0)}) = \lambda_j(\mat{A}) - \gamma i$
%   for $j \neq k$ and
%   $\lambda_k(\tilde{\mat{A}}^{(0)}) = \lambda_k(\mat{A})$.
% \end{lemma}

% \begin{proof}
%   First consider $j \neq k$. A straightforward calculation yields
%   \begin{equation*}
%     (\mat{A} - \gamma i(\mat{I}- \vec{v}_k \vec{v}_k^\tp)) \vec{v}_j = \mat{A}\vec{v}_j - \gamma i \vec{v}_j + \gamma i \vec{v}_k \underbrace{\vec{v}_k^\tp  \vec{v}_j}_{ = 0}
%     = \lambda_j \vec{v}_j - \gamma i \vec{v}_j = (\lambda_j - \gamma i) \vec{v}_j \,.   
%   \end{equation*}
%   For $j = k$ we have
%   $\vec{v}_k^\tp \vec{v}_j = \vec{v}_k^\tp \vec{v}_k = 1$ and thus
%   \begin{equation*}
%     (\mat{A} - \gamma i(\mat{I}- \vec{v}_k \vec{v}_k^\tp)) \vec{v}_k = \lambda_k \vec{v}_k - \gamma i \vec{v}_k + \gamma i \vec{v}_k = \lambda_k \vec{v}_k\,.
%   \end{equation*}
% \end{proof}
% We now decompose $\tilde{\mat{A}}$ into the sum
% $\tilde{\mat{A}} = \tilde{\mat{A}}^{(0)} + \tilde{\mat{A}}^{(1)}$,
% where $\tilde{\mat{A}}^{(0)}$ is defined in
% Lemma~\ref{lem:eigs:atilde0} and $\tilde{\mat{A}}^{(1)}$ is given by
% \begin{equation*}
%   \tilde{\mat{A}}^{(1)} \coloneqq \gamma i ( \vec{u} \vec{u}^\tp - \vec{v}_k\vec{v}_k^\tp)\,.
% \end{equation*}
% \todo{Complete perturbation derivation} Of course, when RQI is
% applied to the matrix $\tilde{\mat{A}}$ the results would not be the
% eigenvalues of $\mat{A}$ but those of $\tilde{\mat{A}}$ . Therefore,
% instead of using a fixed imaginary shift $\gamma i$ we choose a
% decreasing sequence $\gamma^{(k)}$ and perform one step of RQI using
% $\tilde{\mat{A}}^{(k)} \coloneqq \mat{A} - \gamma^{(k)} i(\mat{I} -
% \vec{u} \vec{u}^\tp)$. As $\gamma^{(k)} \rightarrow 0$ the matrix
% $\tilde{\mat{A}}^{(k)}$ approaches $\mat{A}$.



% Weyl`s nequality then gives the bound
% \begin{equation}
%   \label{eq:weyl}
%   \max_{k = 1}^n\, \abs*{\lambda_k(\mat{\tilde{A}}) - \lambda_k(\tilde{\mat{A^{0}}})} \le \norm{\tilde{\mat{A}}^{(1)}}\,.
% \end{equation}
% To compute the norm on the right hand side of~\eqref{eq:weyl} we
% make use of the following well-known facts from linear algebra.
% \begin{proposition}
%   Let $\mat{A} \in \R^{n \times n}$ be a real symmetric matrix. Then
%   \begin{enumerate}[label=(\alph*)]
%   \item The sum of the eigenvalues of $\mat{A}$ is equal to the
%     \emph{trace} of $\mat{A}$ that is defined as the sum of its
%     diagonal entries, \ie,
%     \begin{equation*}
%       \sum_{j = 1}^n \lambda_j(\mat{A}) = \sum_{k = 1}^n \mat{A}_{kk}\,.
%     \end{equation*}
%   \end{enumerate}
% \end{proposition}


% used as an initial guess in the classic Rayleigh Quotient Iteration.
% We can, however, use~\eqref{eq:guess_orthogonal} to shift the
% eigenvalues in such a way that the distance between the target
% eigenvalue and adjacent eigenvalues is increased. This fact is then
% used to alter the linear system that is solved at each step in the
% Rayleigh Quotient Iteration, increasing the chance that the method
% will converge to the right eigenpair.

% We first observe that given a vector $\vec{x} \in \R^n$, the
% projection of $\vec{x} \in \R^n$ onto the span of a unit vector
% $\vec{u}$ is
% \[
%   (\vec{u}^\tp \vec{x})\vec{u} = \vec{u}(\vec{u}^\tp
%   \vec{x})=(\vec{u} \vec{u}^\tp) \vec{x}\,,
% \]
% Thus, the projection onto the orthogonal complement of the span of
% $\vec{u}$ is
% \[
%   \vec{x} - (\vec{u}^\tp \vec{x}) \vec{u} = \vec{x} - (\vec{u}
%   \vec{u}^\tp) \vec{x} = (\mat{I} - \vec{u} \vec{u}^\tp) \vec{x} \,.
% \]
% Consider the matrix
% $\tilde{\mat{A}} \coloneqq \mat{A} - \gamma i ( \mat{I} - \vec{u}
% \vec{u}^\tp)$ where $\gamma > 0$ is an arbitrary real number and $i$
% denotes the imaginary unit. Multiplying this matrix with an
% eigenvector of $\mat{A}$ gives
% \begin{align*}
%   \tilde{\mat{A}}\vec{v}_j &= \left(  \mat{A} - \gamma i ( \mat{I} - \vec{v}_k
%                              \vec{v}_k^\tp) \right) \vec{v}_j \\
%                            &= \mat{A} \vec{v}_j - \gamma i \vec{v}_j + \gamma i \vec{v}_k \vec{v}_k^\tp \vec{v}_j \\
%                            &= (\lambda_j - \gamma i)\vec{v}_j + \gamma i \delta_{kj} \vec{v}_k \,,
% \end{align*}
% where $\delta_{kj}$ denotes the Kronecker delta. In other words, if
% $j \neq k$ we have
% \[
%   \tilde{\mat{A}} \vec{v}_j = (\lambda_j - \gamma i)\vec{v}_j
% \]
% and if $j = k$ we have
% \[
%   \tilde{\mat{A}} \vec{v}_j = \tilde{\mat{A}} \vec{v}_k = (\lambda_k
%   - \gamma i) \vec{v}_k - \gamma i \vec{u} = \lambda_k \vec{v}_k \,.
% \]
% We can conclude that $\tilde{\mat{A}}$ has the same set of
% eigenvectors as $\mat{A}$ with corresponding eigenvalues
% $\lambda_j - \gamma i$ if $j \neq k$ and $\lambda_j$ if $j = k$. In
% other words, we ``raised'' all eigenvalues corresponding to
% eigenvectors that are orthogonal to $\vec{u}$ from the real line
% into the complex plane such that their imaginary parts are equal to
% $\gamma$ increasing the distance \wrt the Euclidean norm between the
% eigenvalues.

% Note, however, that $\vec{u}$ is an eigenvector of $\mat{A}$. We
% will shortly use this approach to guarantee convergence to the
% target eigenvector using a Rayleigh Quotient type Iteration. Of
% course, we will merely have an approximation of an eigenvector so
% that~\eqref{eq:guess_orthogonal} holds. However, as the observations
% above suggest, this approach also works for such
% approximations. This is due to the fact that vectors that lie in the
% orthogonal complement of the span of an approximate eigenvector are
% themselves approximate eigenvectors. The eigenvalues are then raised
% according to the angle between the corresponding eigenvector and the
% subspace defined by the orthogonal projection of the span of
% $\vec{u}$ (see the Lemma below).  Thus, the target eigenvalue (\ie
% the eigenvalue corresponding to the eigenvector closest to
% $\vec{u}$) is barely raised.


% \begin{lemma}\label{lem:eig:orthogprojection}
%   Define the matrix
%   $\tilde{\mat{A}} \coloneqq \mat{A} - \gamma i (\mat{I} -
%   \vec{u}\vec{u}^\tp)$.
% \end{lemma}

% \begin{corollary}
%   The Rayleigh sequences generated using the matrices $\mat{B}$ and
%   $\mat{C}$ as in~\eqref{eq:rayleigh_quotient_complex}
%   and~\eqref{eq:rayleigh_quotient_proj}, respectively, are the same.
% \end{corollary}

% \section{Preconditioners for the Complex Symmetric System}

% \section{Numerical Examples}
% \subsection{Preconditioned Large Sparse System}


%%% Local Variables:
%%% mode: latex 
%%% TeX-master: "../../main"
%%% End:
 